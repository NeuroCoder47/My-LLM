{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "\"vocab_size\": 50257, # Vocabulary size\n",
    "\"context_length\": 1024, # Context length\n",
    "\"emb_dim\": 768, # Embedding dimension\n",
    "\"n_heads\": 12, # Number of attention heads\n",
    "\"n_layers\": 12, # Number of layers\n",
    "\"drop_rate\": 0.1, # Dropout rate\n",
    "\"qkv_bias\": False # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "        *[DummyTransformerBlock(cfg)\n",
    "        for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "        cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(\n",
    "        torch.arange(seq_len, device=in_idx.device)\n",
    "        )\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DummyGPTModel class in this code defines a simplified version of a GPT-like\n",
    "model using PyTorch’s neural network module (nn.Module). The model architecture\n",
    "in the DummyGPTModel class consists of token and positional embeddings, dropout,a series of transformer blocks (DummyTransformerBlock), a final layer normalization(DummyLayerNorm), and a linear output layer (out_head). The configuration ispassed in via a Python dictionary, for instance, the GPT_CONFIG_124M dictionary we created earlier.\n",
    "\n",
    " \n",
    " \n",
    "The forward method describes the data flow through the model: it computes token\n",
    "and positional embeddings for the input indices, applies dropout, processes the datathrough the transformer blocks, applies normalization, and finally produces logits with the linear output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-0.9289,  0.2748, -0.7557,  ..., -1.6070,  0.2702, -0.5888],\n",
      "         [-0.4476,  0.1726,  0.5354,  ..., -0.3932,  1.5285,  0.8557],\n",
      "         [ 0.5680,  1.6053, -0.2155,  ...,  1.1624,  0.1380,  0.7425],\n",
      "         [ 0.0447,  2.4787, -0.8843,  ...,  1.3219, -0.0864, -0.5856]],\n",
      "\n",
      "        [[-1.5474, -0.0542, -1.0571,  ..., -1.8061, -0.4494, -0.6747],\n",
      "         [-0.8422,  0.8243, -0.1098,  ..., -0.1434,  0.2079,  1.2046],\n",
      "         [ 0.1355,  1.1858, -0.1453,  ...,  0.0869, -0.1590,  0.1552],\n",
      "         [ 0.1666, -0.8138,  0.2307,  ...,  2.5035, -0.3055, -0.3083]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits = model(batch)\n",
    "print(\"Output shape:\", logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s now implement layer normalization to improve the stability and efficiency of neu-\n",
    "ral network training. The main idea behind layer normalization is to adjust the activa-\n",
    "tions (outputs) of a neural network layer to have a mean of 0 and a variance of 1, also\n",
    "known as unit variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4597, 0.5231, 0.0000, 0.0000, 0.1215, 0.0000, 0.1660, 0.0000, 0.4517,\n",
      "         0.1732],\n",
      "        [0.4471, 0.4156, 0.0000, 0.0000, 0.5732, 0.1271, 0.1047, 0.0000, 0.0000,\n",
      "         0.2937]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch_example= torch.randn(2,5)\n",
    "layer = nn.Sequential(nn.Linear(5,10),nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is creating and using a simple **neural network layer** in PyTorch. Let’s break it down step by step in an easy way:  \n",
    "\n",
    "---\n",
    "\n",
    "### **1. Setting a Random Seed**\n",
    "```python\n",
    "torch.manual_seed(123)\n",
    "```\n",
    "- This makes sure that every time we run the code, we get the same random numbers.  \n",
    "- It is useful for reproducibility (so results don’t change every time you run the code).  \n",
    "\n",
    "---\n",
    "\n",
    "### **2. Creating a Random Input Batch**\n",
    "```python\n",
    "batch_example = torch.randn(2, 5)\n",
    "```\n",
    "- `torch.randn(2, 5)` creates a **2×5 matrix** filled with random numbers from a normal distribution.  \n",
    "- This represents a **batch** of 2 samples, each having **5 features**.  \n",
    "- Example (random numbers will be different):  \n",
    "  ```\n",
    "  [[ 1.23, -0.45,  0.67, -1.09,  0.82],\n",
    "   [-0.67,  1.45, -0.89,  0.33, -0.76]]\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Creating a Simple Neural Network Layer**\n",
    "```python\n",
    "layer = nn.Sequential(nn.Linear(5, 10), nn.ReLU())\n",
    "```\n",
    "- `nn.Sequential(...)` stacks layers together. Here, we have:  \n",
    "  1. `nn.Linear(5, 10)`:  \n",
    "     - A **fully connected (linear) layer** that takes 5 input values and produces 10 output values.  \n",
    "     - It does this by multiplying the input by a **weight matrix** and adding a **bias**.  \n",
    "  2. `nn.ReLU()`:  \n",
    "     - A **ReLU (Rectified Linear Unit) activation function**, which replaces all negative values with **0**.  \n",
    "     - It helps the model learn **non-linear patterns**.  \n",
    "\n",
    "---\n",
    "\n",
    "### **4. Passing Data Through the Layer**\n",
    "```python\n",
    "out = layer(batch_example)\n",
    "```\n",
    "- The `batch_example` (which has a shape of **(2,5)**) is passed through the `layer`.  \n",
    "- The **linear layer** transforms it into a shape of **(2,10)** (since it outputs 10 values).  \n",
    "- The **ReLU activation** sets all negative values to zero.  \n",
    "\n",
    "---\n",
    "\n",
    "### **5. Printing the Output**\n",
    "```python\n",
    "print(out)\n",
    "```\n",
    "- The final output is a **2×10 matrix** with transformed values.  \n",
    "- Example output (values will be different due to randomness):  \n",
    "  ```\n",
    "  tensor([[0.54, 0.00, 1.23, 0.00, 0.98, 0.00, 0.00, 0.76, 0.00, 0.12],\n",
    "          [0.00, 0.67, 0.00, 0.89, 0.00, 0.34, 1.45, 0.00, 0.98, 0.00]])\n",
    "  ```\n",
    "- Notice how some values are **zero** because of the **ReLU activation function**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Final Summary**\n",
    "- We created **random input data** (2 samples, each with 5 features).  \n",
    "- We defined a **simple neural network layer** (Linear transformation + ReLU activation).  \n",
    "- We passed the input through this layer, getting a **transformed output**.  \n",
    "- The output is a **2×10 matrix**, where some values are set to **zero** by ReLU.  \n",
    "\n",
    "This is a small part of how deep learning models work! 🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean :  tensor([[0.1895],\n",
      "        [0.1961]], grad_fn=<MeanBackward1>)\n",
      "Varienace :  tensor([[0.0447],\n",
      "        [0.0478]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = out.mean(dim = 1 , keepdim= True)\n",
    "var =  out.var(dim = 1 , keepdim = True)\n",
    "print(\"Mean : \", mean)\n",
    "print(\"Varienace : \", var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized layer outputs:\n",
      " tensor([[ 1.2783,  1.5781, -0.8966, -0.8966, -0.3217, -0.8966, -0.1114, -0.8966,\n",
      "          1.2404, -0.0774],\n",
      "        [ 1.1482,  1.0041, -0.8975, -0.8975,  1.7255, -0.3157, -0.4186, -0.8975,\n",
      "         -0.8975,  0.4465]], grad_fn=<DivBackward0>)\n",
      "Mean:\n",
      " tensor([[ 5.9605e-08],\n",
      "        [-2.3842e-08]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out_norm = (out - mean)/torch.sqrt(var)\n",
    "mean = out_norm.mean(dim = -1, keepdim = True)\n",
    "var = out_norm.var(dim = -1, keepdim = True)\n",
    "print(\"Normalized layer outputs:\\n\", out_norm)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "    def forward(self,x):\n",
    "        mean = x.mean(dim = -1, keepdim = True)\n",
    "        var = x.var(dim = -1, keepdim = True)\n",
    "        norm_x=(x-mean)/ torch.sqrt(var+self.eps)\n",
    "        return norm_x*self.scale+self.shift\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This specific implementation of layer normalization operates on the last dimension of the input tensor x, which represents the embedding dimension (emb_dim). \n",
    "\n",
    "The vari-able eps is a small constant (epsilon) added to the variance to prevent division by zeroduring normalization. \n",
    "\n",
    "\n",
    "The scale and shift are two trainable parameters (of thesame dimension as the input) that the LLM automatically adjusts during training if it\n",
    "is determined that doing so would improve the model’s performance on its training\n",
    "task.\n",
    "\n",
    " This allows the model to learn appropriate scaling and shifting that best suit thedata it is processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines a **custom Layer Normalization (LayerNorm) module** in PyTorch. Let’s break it down step by step in simple terms.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. What is Layer Normalization?**\n",
    "Layer Normalization is a technique that **normalizes** the inputs across their features. This helps stabilize and speed up training in deep learning models.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Breaking Down the Code**\n",
    "### **Step 1: Creating a Custom Neural Network Module**\n",
    "```python\n",
    "class LayerNorm(nn.Module):\n",
    "```\n",
    "- This defines a new class called `LayerNorm`, which is a custom **neural network layer**.  \n",
    "- It **inherits** from `nn.Module`, which is the base class for all PyTorch models.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Initializing the Layer**\n",
    "```python\n",
    "def __init__(self, emb_dim):\n",
    "    super().__init__()\n",
    "    self.eps = 1e-5\n",
    "    self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "    self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "```\n",
    "- This is the **constructor** (`__init__` method), which runs when we create a `LayerNorm` object.\n",
    "- **`emb_dim`** is the number of input features (i.e., how many values each input has).  \n",
    "- **`self.eps = 1e-5`**:  \n",
    "  - A small constant added to prevent division by zero.  \n",
    "- **`self.scale` and `self.shift`**:  \n",
    "  - These are **trainable parameters** that allow the model to adjust the normalization.  \n",
    "  - `scale` (γ) starts as **ones** (so it doesn’t change the input initially).  \n",
    "  - `shift` (β) starts as **zeros** (so it doesn’t add any offset initially).  \n",
    "  - `nn.Parameter(...)` makes them **learnable** during training.\n",
    "\n",
    "> **⚠️ Mistake:** The `__init__` method is missing a `_` in `__init(self, emb_dim)`. It should be `__init__(self, emb_dim)`.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3: Forward Pass (Applying Layer Normalization)**\n",
    "```python\n",
    "def forward(self, x):\n",
    "    mean = x.mean(dim=-1, keepdim=True)\n",
    "    var = x.var(dim=-1, keepdim=True)\n",
    "    norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "    return norm_x * self.scale + self.shift\n",
    "```\n",
    "This method defines how the input `x` is transformed when passed through the layer.  \n",
    "\n",
    "#### **Breaking it down:**\n",
    "1. **Compute the Mean:**\n",
    "   ```python\n",
    "   mean = x.mean(dim=-1, keepdim=True)\n",
    "   ```\n",
    "   - Calculates the **mean** of `x` along the last dimension (`dim=-1`).\n",
    "   - `keepdim=True` ensures that the result has the same shape as `x`, making broadcasting easier.\n",
    "\n",
    "2. **Compute the Variance:**\n",
    "   ```python\n",
    "   var = x.var(dim=-1, keepdim=True)\n",
    "   ```\n",
    "   - Calculates the **variance** of `x` along the last dimension.\n",
    "\n",
    "3. **Normalize the Input:**\n",
    "   ```python\n",
    "   norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "   ```\n",
    "   - Subtracts the mean and divides by the **standard deviation** (`sqrt(variance)`).\n",
    "   - This **scales** the data so that it has a mean of 0 and a variance of 1.\n",
    "\n",
    "4. **Apply Learnable Parameters (Scale and Shift):**\n",
    "   ```python\n",
    "   return norm_x * self.scale + self.shift\n",
    "   ```\n",
    "   - Multiplies by `self.scale` (γ) and adds `self.shift` (β).\n",
    "   - This allows the network to **adjust the normalization** during training.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Summary**\n",
    "🔹 **What does this LayerNorm class do?**\n",
    "- Normalizes each sample **independently** across its features.  \n",
    "- Ensures a mean of **0** and variance of **1**.  \n",
    "- Uses **trainable parameters** (`scale` and `shift`) to allow flexibility.  \n",
    "\n",
    "🔹 **Why use Layer Normalization?**\n",
    "- It helps **stabilize** training, especially in **transformers** and recurrent networks.  \n",
    "- Unlike **BatchNorm**, it does **not** depend on batch size (so it works well in small batches).  \n",
    "\n",
    "This is a simplified version of PyTorch’s built-in `nn.LayerNorm`! 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GELU Activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class GELU(nn.Module):\n",
    "    def forward(self,x):\n",
    "        return 0.5*0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2.0 / torch.pi)) *(x + 0.044715 * torch.pow(x, 3))))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "gelu, relu = GELU(), nn.ReLU()\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "y_gelu, y_relu = gelu(x), relu(x)\n",
    "plt.figure(figsize=(8, 3))\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n",
    "    plt.subplot(1, 2, i)\n",
    "    plt.plot(x, y)\n",
    "    plt.title(f\"{label} activation function\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(f\"{label}(x)\")\n",
    "    plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break down the provided code step by step to understand its components and functionality:\n",
    "\n",
    "```python\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "```\n",
    "\n",
    "1. **Defining the `FeedForward` Class**:\n",
    "   - `class FeedForward(nn.Module):`\n",
    "     - This line defines a new class named `FeedForward` that inherits from `nn.Module`, which is the base class for all neural network modules in PyTorch. Inheriting from `nn.Module` allows us to leverage PyTorch's built-in functionalities for neural networks.\n",
    "\n",
    "2. **Initialization Method (`__init__`)**:\n",
    "   - `def __init__(self, cfg):`\n",
    "     - This is the constructor method that initializes the `FeedForward` class. It takes `cfg` as an argument, which is expected to be a dictionary containing configuration parameters.\n",
    "\n",
    "   - `super().__init__()`\n",
    "     - This line calls the constructor of the parent class (`nn.Module`) to ensure that the base class is properly initialized.\n",
    "\n",
    "   - `self.layers = nn.Sequential(...)`\n",
    "     - Here, we define a sequence of layers using `nn.Sequential`, which is a container module that processes inputs through a sequence of sub-modules in the order they are added.\n",
    "\n",
    "3. **Defining the Sequential Layers**:\n",
    "   - `nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),`\n",
    "     - This creates a linear (fully connected) layer that takes an input of size `cfg[\"emb_dim\"]` and outputs a tensor of size `4 * cfg[\"emb_dim\"]`. The `nn.Linear` module applies a linear transformation to the incoming data.\n",
    "\n",
    "   - `GELU(),`\n",
    "     - This applies the Gaussian Error Linear Unit (GELU) activation function, which is commonly used in transformer models. The GELU activation function is defined as:\n",
    "\n",
    "       \\[ \\text{GELU}(x) = x \\cdot \\Phi(x) \\]\n",
    "\n",
    "       where \\( \\Phi(x) \\) is the cumulative distribution function of the standard normal distribution.\n",
    "\n",
    "   - `nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),`\n",
    "     - This adds another linear layer that reduces the dimensionality back from `4 * cfg[\"emb_dim\"]` to `cfg[\"emb_dim\"]`.\n",
    "\n",
    "4. **Forward Method**:\n",
    "   - `def forward(self, x):`\n",
    "     - This method defines the forward pass of the network. It takes an input tensor `x` and passes it through the sequence of layers defined in `self.layers`.\n",
    "\n",
    "   - `return self.layers(x)`\n",
    "     - This line applies the sequential layers to the input `x` and returns the output.\n",
    "\n",
    "**Summary**:\n",
    "The `FeedForward` class represents a feedforward neural network module commonly used in transformer architectures. It consists of two linear transformations with a GELU activation function in between. The first linear layer expands the input dimensionality by a factor of four, the GELU activation introduces non-linearity, and the second linear layer projects the dimensionality back to its original size. This structure allows the model to learn complex representations and is a standard component in transformer-based models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "x = torch.rand(2, 3, 768)\n",
    "out = ffn(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShortcutConnection(nn.Module):\n",
    "    def __init__(self,layer_size,use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_size[0] , layer_size[1]),\n",
    "                         GELU() ),\n",
    "             nn.Sequential(nn.Linear(layer_size[0] , layer_size[1]),\n",
    "                         GELU() ),\n",
    "             nn.Sequential(nn.Linear(layer_size[0] , layer_size[1]),\n",
    "                         GELU() ),\n",
    "             nn.Sequential(nn.Linear(layer_size[0] , layer_size[1]),\n",
    "                         GELU() ),\n",
    "        ])\n",
    "    def forward(self,x):\n",
    "            for layer in self.layers:\n",
    "                layer_output = layer(x)\n",
    "                if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                    x = x + layer_output\n",
    "                else:\n",
    "                    x = layer_output\n",
    "            return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.10845032334327698\n",
      "layers.1.0.weight has gradient mean of 0.07727959007024765\n",
      "layers.2.0.weight has gradient mean of 0.126175656914711\n",
      "layers.3.0.weight has gradient mean of 0.1201065257191658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashmi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "torch.manual_seed(123)\n",
    "model_without_shortcut = ShortcutConnection(\n",
    "layer_sizes, use_shortcut=False\n",
    ")\n",
    "\n",
    "\n",
    "def print_gradients(model, x):\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "    loss.backward()\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model_with_shortcut = ShortcutConnection(\n",
    "layer_sizes, use_shortcut=True\n",
    ")\n",
    "print_gradients(model_with_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulatHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out,context_length , dropout, num_heads, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads \n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask' , torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "    def forward(self , x):\n",
    "        b, num_tokens,d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        keys = keys.view(b, num_tokens,self.num_heads,self.head_dim)\n",
    "        values = values.view(b, num_tokens,self.num_heads,self.head_dim)\n",
    "        queries = queries.view(b, num_tokens,self.num_heads,self.head_dim)\n",
    "        keys = keys.transpose(1,2)\n",
    "        queries = queries.transpose(1,2)\n",
    "        values = values.transpose(1,2)\n",
    "        attn_scores = queries @ keys.transpose(-2,-1)\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vec= (attn_weights@values).transpose(1,2)\n",
    "        contex_vec  = context_vec.contigious().view(b, num_tokens,self.d_out)\n",
    "        contex_vec= contex_vec.out_proj(contex_vec)\n",
    "        return contex_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MulatHeadAttention(\n",
    "             d_in=cfg[\"emb_dim\"],\n",
    "             d_out=cfg[\"emb_dim\"],\n",
    "             context_length=cfg[\"context_length\"],\n",
    "             num_heads=cfg[\"n_heads\"],\n",
    "             dropout=cfg[\"drop_rate\"],\n",
    "             qkv_bias=cfg[\"qkv_bias\"] \n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "    def forward(self,x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'contigious'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m768\u001b[39m)\n\u001b[0;32m      3\u001b[0m block \u001b[38;5;241m=\u001b[39m TransformerBlock(GPT_CONFIG_124M)\n\u001b[1;32m----> 4\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, output\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[42], line 19\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     17\u001b[0m shortcut \u001b[38;5;241m=\u001b[39m x\n\u001b[0;32m     18\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x)\n\u001b[1;32m---> 19\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43matt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_shortcut(x)\n\u001b[0;32m     21\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m shortcut\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[41], line 32\u001b[0m, in \u001b[0;36mMulatHeadAttention.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     30\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attn_weights)\n\u001b[0;32m     31\u001b[0m context_vec\u001b[38;5;241m=\u001b[39m (attn_weights\u001b[38;5;129m@values\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 32\u001b[0m contex_vec  \u001b[38;5;241m=\u001b[39m \u001b[43mcontext_vec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontigious\u001b[49m()\u001b[38;5;241m.\u001b[39mview(b, num_tokens,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_out)\n\u001b[0;32m     33\u001b[0m contex_vec\u001b[38;5;241m=\u001b[39m contex_vec\u001b[38;5;241m.\u001b[39mout_proj(contex_vec)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m contex_vec\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'contigious'"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768)\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
