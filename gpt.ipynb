{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wd_cuJfHRvAQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "import numpy as np\n",
        "import os\n",
        "import sentencepiece as spm\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from contextlib import contextmanager\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, file_path, tokenizer, sequence_length=64):\n",
        "        self.sequence_length = sequence_length\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        # Read and tokenize full text\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            text = f.read()\n",
        "\n",
        "        # Encode full text\n",
        "        self.tokens = self.tokenizer.encode_as_ids(text)\n",
        "        self.num_sequences = len(self.tokens) - sequence_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_sequences\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sequence = self.tokens[idx:idx + self.sequence_length]\n",
        "        target = self.tokens[idx + 1:idx + self.sequence_length + 1]\n",
        "        return torch.tensor(sequence), torch.tensor(target)\n",
        "\n",
        "class SinusoidalPosEmb(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        device = x.device\n",
        "        half_dim = self.dim // 2\n",
        "        emb = np.log(10000) / (half_dim - 1)\n",
        "        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)\n",
        "        emb = x[:, None] * emb[None, :]\n",
        "        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
        "        return emb\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, hidden_size=128, num_heads=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(hidden_size)\n",
        "        self.attention = nn.MultiheadAttention(\n",
        "            hidden_size, num_heads, dropout=dropout, batch_first=True\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(hidden_size)\n",
        "        self.feedforward = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size * 4),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_size * 4, hidden_size),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, padding_mask=None):\n",
        "        seq_len = x.shape[1]\n",
        "        causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=x.device), 1).bool()\n",
        "\n",
        "        attended = self.attention(\n",
        "            self.norm1(x), self.norm1(x), self.norm1(x),\n",
        "            attn_mask=causal_mask,\n",
        "            key_padding_mask=padding_mask\n",
        "        )[0]\n",
        "        x = x + attended\n",
        "\n",
        "        x = x + self.feedforward(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "class TextTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size=256, num_layers=6, num_heads=8, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.pos_emb = SinusoidalPosEmb(hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.transformer_blocks = nn.ModuleList([\n",
        "            TransformerBlock(hidden_size, num_heads, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(hidden_size)\n",
        "        self.output = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, padding_mask=None):\n",
        "        b, s = x.shape\n",
        "        token_emb = self.embedding(x)\n",
        "        pos_ids = torch.arange(s, device=x.device)\n",
        "        pos_emb = self.pos_emb(pos_ids).unsqueeze(0).expand(b, -1, -1)\n",
        "        x = self.dropout(token_emb + pos_emb)\n",
        "\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x, padding_mask)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        return self.output(x)\n",
        "\n",
        "@contextmanager\n",
        "def nullcontext():\n",
        "    yield\n",
        "\n",
        "def train_tokenizer(file_path, vocab_size=8000, model_prefix=\"spm_model\"):\n",
        "    \"\"\"Train a SentencePiece tokenizer on the input data.\"\"\"\n",
        "    print(\"Training SentencePiece tokenizer...\")\n",
        "\n",
        "    # Create temporary file with one sentence per line\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "\n",
        "    temp_file = \"temp_training_data.txt\"\n",
        "    with open(temp_file, 'w', encoding='utf-8') as f:\n",
        "        # Simple sentence splitting on periods\n",
        "        sentences = text.replace('\\n', ' ').split('.')\n",
        "        for sentence in sentences:\n",
        "            if sentence.strip():\n",
        "                f.write(sentence.strip() + '.\\n')\n",
        "\n",
        "    # Train SentencePiece model\n",
        "    spm.SentencePieceTrainer.train(\n",
        "        f'--input={temp_file} --model_prefix={model_prefix} '\n",
        "        f'--vocab_size={vocab_size} --character_coverage=1.0 '\n",
        "        '--model_type=bpe --pad_id=0 --unk_id=1 --bos_id=2 --eos_id=3 '\n",
        "        '--pad_piece=<pad> --unk_piece=<unk> --bos_piece=<s> --eos_piece=</s>'\n",
        "    )\n",
        "\n",
        "    # Clean up temporary file\n",
        "    os.remove(temp_file)\n",
        "\n",
        "    # Load the trained model\n",
        "    sp = spm.SentencePieceProcessor()\n",
        "    sp.load(f\"{model_prefix}.model\")\n",
        "    return sp\n",
        "\n",
        "def train_transformer(\n",
        "    file_path,\n",
        "    sequence_length=64,\n",
        "    hidden_size=256,\n",
        "    num_layers=6,\n",
        "    num_heads=8,\n",
        "    batch_size=32,\n",
        "    learning_rate=3e-4,\n",
        "    num_epochs=10,\n",
        "    device=None\n",
        "):\n",
        "    \"\"\"Train the transformer with SentencePiece tokenization.\"\"\"\n",
        "\n",
        "    if device is None:\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Train and load tokenizer\n",
        "    tokenizer = train_tokenizer(file_path)\n",
        "    vocab_size = tokenizer.get_piece_size()\n",
        "\n",
        "    # Create dataset and dataloader\n",
        "    dataset = TextDataset(file_path, tokenizer, sequence_length)\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        drop_last=True,\n",
        "        pin_memory=True if device.type == 'cuda' else False,\n",
        "        num_workers=4 if device.type == 'cuda' else 0\n",
        "    )\n",
        "\n",
        "    # Initialize model\n",
        "    model = TextTransformer(\n",
        "        vocab_size=vocab_size,\n",
        "        hidden_size=hidden_size,\n",
        "        num_layers=num_layers,\n",
        "        num_heads=num_heads\n",
        "    ).to(device)\n",
        "\n",
        "    # Initialize optimizer and training components\n",
        "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "    scaler = torch.cuda.amp.GradScaler() if device.type == 'cuda' else None\n",
        "\n",
        "    print(f\"\\nTraining on {device}\")\n",
        "    print(f\"Vocabulary size: {vocab_size}\")\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0\n",
        "        progress_bar = tqdm(\n",
        "            total=len(dataloader),\n",
        "            desc=f\"Epoch {epoch + 1}/{num_epochs}\",\n",
        "            unit=\"batch\"\n",
        "        )\n",
        "\n",
        "        for batch_idx, (sequences, targets) in enumerate(dataloader):\n",
        "            sequences = sequences.to(device, non_blocking=True)\n",
        "            targets = targets.to(device, non_blocking=True)\n",
        "\n",
        "            if device.type == 'cuda':\n",
        "                with torch.cuda.amp.autocast():\n",
        "                    outputs = model(sequences)\n",
        "                    loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
        "            else:\n",
        "                outputs = model(sequences)\n",
        "                loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            if device.type == 'cuda':\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "                optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            avg_loss = epoch_loss / (batch_idx + 1)\n",
        "\n",
        "            progress_bar.set_postfix({\n",
        "                'loss': f'{loss.item():.4f}',\n",
        "                'avg_loss': f'{avg_loss:.4f}'\n",
        "            })\n",
        "            progress_bar.update(1)\n",
        "\n",
        "        progress_bar.close()\n",
        "        print(f\"\\nEpoch {epoch + 1} completed - Average Loss: {avg_loss:.4f}\\n\")\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "def generate_text(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt,\n",
        "    max_length=100,\n",
        "    temperature=0.7,\n",
        "    device=None\n",
        "):\n",
        "    \"\"\"Generate text using the trained model and SentencePiece tokenizer.\"\"\"\n",
        "    if device is None:\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Encode prompt\n",
        "    tokens = tokenizer.encode_as_ids(prompt)\n",
        "    tokens = torch.tensor(tokens).unsqueeze(0).to(device)\n",
        "\n",
        "    # Generate text\n",
        "    with torch.no_grad():\n",
        "        with torch.cuda.amp.autocast() if device.type == 'cuda' else nullcontext():\n",
        "            for _ in range(max_length):\n",
        "                outputs = model(tokens)\n",
        "                next_token_logits = outputs[:, -1, :] / temperature\n",
        "                probs = F.softmax(next_token_logits, dim=-1)\n",
        "                next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "                if next_token.item() == tokenizer.eos_id():\n",
        "                    break\n",
        "\n",
        "                tokens = torch.cat([tokens, next_token], dim=1)\n",
        "\n",
        "    # Decode and return generated text\n",
        "    generated_tokens = tokens.squeeze().cpu().tolist()\n",
        "    return tokenizer.decode_ids(generated_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = train_transformer(\n",
        "    file_path=\"train.txt\",\n",
        "    sequence_length=128,  # Increased from 64 to capture more context\n",
        "    hidden_size=512,      # Increased from 256 for more capacity\n",
        "    num_layers=8,         # Increased from 6 for deeper processing\n",
        "    num_heads=8,          # Keep same number of heads\n",
        "    batch_size=96,        # Reduced to help with larger model\n",
        "    num_epochs=20,        # Increased training time\n",
        "    learning_rate=1e-4    # Slightly lower learning rate for stability\n",
        ")\n",
        "# Generate text (function remains the same but uses the BPE tokenizer)\n",
        "generated_text = generate_text(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt=\"who is this\",\n",
        "    max_length=100,\n",
        "    temperature=0.7\n",
        ")\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCgdaQaGXZzl",
        "outputId": "3eced000-c4f2-4aac-a46c-7d0ae295e948"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training SentencePiece tokenizer...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/20:   1%|          | 127/23068 [00:43<2:11:15,  2.91batch/s, loss=6.7151, avg_loss=7.1964]\n",
            "<ipython-input-4-f0e4974fdb81>:184: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler() if device.type == 'cuda' else None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training on cuda\n",
            "Vocabulary size: 8000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/20:   0%|          | 0/7689 [00:00<?, ?batch/s]<ipython-input-4-f0e4974fdb81>:203: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "Epoch 1/20:  19%|█▉        | 1486/7689 [06:27<27:23,  3.77batch/s, loss=4.2185, avg_loss=5.2177]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text = generate_text(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt=\"Once upon a time\",\n",
        "    max_length=100,\n",
        "    temperature=0.7\n",
        ")\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmSUZzdUu-nm",
        "outputId": "c7dce28e-8abe-47cb-f0d2-307f2d5a291e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-f0e4974fdb81>:256: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast() if device.type == 'cuda' else nullcontext():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ⁇ nce upon a time that i was able to capture my first show for my study on the site. ian may suggest reading the theme of the bengali dim, without more information, it is impossible to determine whether it leisure became a man. the man is on the moon and the hanksb. the doorway, and the three cruise of bread for a cabinet is the projecting on the project and smiling a computer. this cultural exchange, and a 31,000 a 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model and tokenizer\n",
        "torch.save({'model': model.state_dict(), 'tokenizer': tokenizer}, 'model.pt')\n",
        "\n",
        "# Load model and tokenizer\n",
        "checkpoint = torch.load('model.pt')\n",
        "model.load_state_dict(checkpoint['model'])\n",
        "tokenizer = checkpoint['tokenizer']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3hVxMtMwuDu",
        "outputId": "bc0cfa46-08ac-4ff3-93ec-ecf6557bb4ab"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-e0127aaacf6e>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load('model.pt')\n"
          ]
        }
      ]
    }
  ]
}