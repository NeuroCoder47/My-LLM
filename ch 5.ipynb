{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Chapter 1 - 4 of Book \"Learning Large Language Models From Scratch (by Sebastian Raschka)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "\"vocab_size\": 50257, # Vocabulary size\n",
    "\"context_length\": 256, # Context length\n",
    "\"emb_dim\": 768, # Embedding dimension\n",
    "\"n_heads\": 12, # Number of attention heads\n",
    "\"n_layers\": 12, # Number of layers\n",
    "\"drop_rate\": 0.1, # Dropout rate\n",
    "\"qkv_bias\": False # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First batch: [tensor([[ 5962, 22307,    25,   198]]), tensor([[22307,    25,   198,  8421]])]\n",
      "Second batch: [tensor([[  25,  198, 8421,  356]]), tensor([[ 198, 8421,  356, 5120]])]\n",
      "Token Embeddings Shape: torch.Size([1, 4, 256])\n",
      "Positional Embeddings Shape: torch.Size([1, 4, 256])\n",
      "Final Input Embeddings Shape: torch.Size([1, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "# ch 1 & 2\n",
    "import urllib.request\n",
    "import torch\n",
    "import tiktoken  \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Download the text file\n",
    "url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "       \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "       \"the-verdict.txt\")\n",
    "file_path = \"the-verdict.txt\"\n",
    "urllib.request.urlretrieve(url, file_path)\n",
    "\n",
    "# Load and tokenize the text\n",
    "with open(r\"C:\\Users\\ashmi\\Documents\\Artificial Intelligence\\Deep Learning\\Pytorch\\Learning Codes\\Deep learning\\PRACTICE\\Transformer\\mama_project\\input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")  \n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "\n",
    "# Define a custom dataset class\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "# Function to create a DataLoader\n",
    "def create_dataloader_v1(txt, batch_size, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "    return dataloader \n",
    "\n",
    "# Create DataLoader\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=1, max_length=4, stride=2, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "second_batch = next(data_iter)\n",
    "print(\"First batch:\", first_batch)\n",
    "print(\"Second batch:\", second_batch)\n",
    "\n",
    "# --- Adding Token and Positional Embeddings ---\n",
    "\n",
    "# Define token embedding layer\n",
    "vocab_size = 50257  # Typical size for GPT models\n",
    "embedding_dim = 256  # Example embedding size (GPT-3 uses 12,288)\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "# Convert token IDs into token embeddings\n",
    "token_embeddings = token_embedding_layer(first_batch[0])  # First batch of inputs\n",
    "print(\"Token Embeddings Shape:\", token_embeddings.shape)\n",
    "\n",
    "# Define positional embedding layer\n",
    "max_length = 4  # Same as the max sequence length\n",
    "pos_embedding_layer = torch.nn.Embedding(max_length, embedding_dim)\n",
    "\n",
    "# Generate position embeddings\n",
    "positions = torch.arange(max_length).unsqueeze(0)  # Create position indices\n",
    "pos_embeddings = pos_embedding_layer(positions)\n",
    "print(\"Positional Embeddings Shape:\", pos_embeddings.shape)\n",
    "\n",
    "# Combine token and positional embeddings\n",
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(\"Final Input Embeddings Shape:\", input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ch 3\n",
    "import torch.nn as nn # type: ignore\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out,context_length , dropout, num_heads, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads \n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask' , torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "    def forward(self , x):\n",
    "        b, num_tokens,d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        keys = keys.view(b, num_tokens,self.num_heads,self.head_dim)\n",
    "        values = values.view(b, num_tokens,self.num_heads,self.head_dim)\n",
    "        queries = queries.view(b, num_tokens,self.num_heads,self.head_dim)\n",
    "        keys = keys.transpose(1,2)\n",
    "        queries = queries.transpose(1,2)\n",
    "        values = values.transpose(1,2)\n",
    "        attn_scores = queries @ keys.transpose(-2,-1)\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)  # Correct method call\n",
    "        return context_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ch 4\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "    def forward(self,x):\n",
    "        mean = x.mean(dim = -1, keepdim = True)\n",
    "        var = x.var(dim = -1, keepdim = True)\n",
    "        norm_x=(x-mean)/ torch.sqrt(var+self.eps)\n",
    "        return norm_x*self.scale+self.shift\n",
    "class GELU(nn.Module):\n",
    "    def forward(self,x):\n",
    "        return 0.5*0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2.0 / torch.pi)) *(x + 0.044715 * torch.pow(x, 3))))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.layers(x)\n",
    "class ShortcutConnection(nn.Module):\n",
    "    def __init__(self,layer_size,use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_size[0] , layer_size[1]),\n",
    "                         GELU() ),\n",
    "             nn.Sequential(nn.Linear(layer_size[0] , layer_size[1]),\n",
    "                         GELU() ),\n",
    "             nn.Sequential(nn.Linear(layer_size[0] , layer_size[1]),\n",
    "                         GELU() ),\n",
    "             nn.Sequential(nn.Linear(layer_size[0] , layer_size[1]),\n",
    "                         GELU() ),\n",
    "        ])\n",
    "    def forward(self,x):\n",
    "            for layer in self.layers:\n",
    "                layer_output = layer(x)\n",
    "                if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                    x = x + layer_output\n",
    "                else:\n",
    "                    x = layer_output\n",
    "            return x\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "             d_in=cfg[\"emb_dim\"],\n",
    "             d_out=cfg[\"emb_dim\"],\n",
    "             context_length=cfg[\"context_length\"],\n",
    "             num_heads=cfg[\"n_heads\"],\n",
    "             dropout=cfg[\"drop_rate\"],\n",
    "             qkv_bias=cfg[\"qkv_bias\"] \n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "    def forward(self,x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "        return x\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
    "        )\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embds = self.tok_emb(in_idx)\n",
    "        pos_embds = self.pos_emb(\n",
    "            torch.arange(seq_len, device=in_idx.device)\n",
    "        )\n",
    "        x= tok_embds + pos_embds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits= self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    for _ in range(max_new_tokens):\n",
    "        # Adjust context size to match the available tokens\n",
    "        start_idx = max(0, idx.size(1) - context_size)\n",
    "        idx_cond = idx[:, start_idx:]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        \n",
    "        logits = logits[:, -1, :]  # Get logits for the last token in context\n",
    "        probas = torch.softmax(logits, dim=-1)  # Convert logits to probabilities\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # Get the next token\n",
    "        \n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # Append the next token to the sequence\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken  # type: ignore\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-0.3877,  0.4158, -0.5787,  ...,  0.1276, -0.5383,  0.2446],\n",
      "         [-0.2554, -0.1288, -0.7926,  ...,  0.3047, -1.1738,  0.3123],\n",
      "         [ 0.7388, -0.1012, -0.4025,  ...,  0.4566, -0.5240,  0.2286],\n",
      "         [-0.0734, -0.7353, -1.0307,  ...,  0.3615,  0.3656, -0.1678]],\n",
      "\n",
      "        [[-0.1286,  0.2205, -0.9525,  ..., -0.1467, -0.7149, -0.0383],\n",
      "         [-0.6371, -0.4865, -0.9413,  ...,  0.2877, -0.2713,  0.2986],\n",
      "         [ 0.3656,  0.0895,  0.2042,  ...,  0.0988,  0.2296, -0.1554],\n",
      "         [-0.2582, -0.1317, -0.5016,  ..., -0.1865, -0.0638, -0.3825]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "out = model(batch)\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"\\nOutput shape:\", out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the phases shown in the text and their summaries:\n",
    "\n",
    "Phase 1 - Model Setup\n",
    "- Initialize GPT model with GPT_CONFIG_124M configuration\n",
    "- Reduce context length to 256 tokens\n",
    "- Set model to evaluation mode\n",
    "\n",
    "Phase 2 - Text Generation Pipeline\n",
    "- Convert input text to token IDs\n",
    "- Process token IDs through model to get logits \n",
    "- Convert logits back to text via token IDs\n",
    "\n",
    "The text ends before showing the implementation of Phase 2, indicating there would be code in the following listing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you renting torch Dur extravagant Vaughn imprisoned conqu Burn plasticsfigured\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Generate text\n",
    "start_context = \"Every effort moves you\"\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "# Print generated text\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, let's break this down step by step in simple terms without skipping any important details.\n",
    "\n",
    "### What is being discussed?\n",
    "\n",
    "This is about evaluating how well a text-generation model (like GPT) works during training by measuring **loss**—a number that tells us how far the model's generated text is from what we want it to generate. This is part of the process of teaching the model to improve its predictions. \n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Loading Data and Text Generation Process**\n",
    "We start with input examples, like sentences, which are first broken into tokens (small pieces of text like words or characters). Each token is converted to a number (called a token ID) using a **vocabulary**—a dictionary that maps text to numbers.\n",
    "\n",
    "#### Example:\n",
    "- Input sentences: `\"every effort moves\"` and `\"I really like\"`\n",
    "- Token IDs:  \n",
    "  ```python\n",
    "  inputs = torch.tensor([\n",
    "      [16833, 3626, 6100],  # \"every effort moves\"\n",
    "      [40, 1107, 588]       # \"I really like\"\n",
    "  ])\n",
    "  ```\n",
    "\n",
    "The target tokens are the **next words** in the sentences, shifted one step forward:\n",
    "- Targets:  \n",
    "  ```python\n",
    "  targets = torch.tensor([\n",
    "      [3626, 6100, 345],   # \"effort moves you\"\n",
    "      [1107, 588, 11311]   # \"really like chocolate\"\n",
    "  ])\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **How the Model Generates Text**\n",
    "The process involves the following steps (illustrated in **Figure 5.4**):\n",
    "\n",
    "1. **Map input text to token IDs**: Use the vocabulary to turn words into numbers.\n",
    "2. **Generate probability scores**: The model predicts how likely each word in its vocabulary is to come next in the sentence. For example:\n",
    "   - For the token `\"every\"`, it might predict:\n",
    "     ```python\n",
    "     [0.10, 0.60, 0.20, 0.05, 0.00, 0.02, 0.01]\n",
    "     ```\n",
    "     This is a probability distribution over all words in the vocabulary.\n",
    "3. **Pick the most likely token**: Use `argmax` to find the token with the highest probability (e.g., `\"effort\"` if its probability is 0.60).\n",
    "4. **Convert back to text**: Map the selected token IDs back into words using an **inverse vocabulary**.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Why Does the Model Generate Random Text Initially?**\n",
    "When the model hasn’t been trained yet, it produces random text because it doesn’t \"know\" what comes next. For example:\n",
    "- Input: `\"every effort moves\"`\n",
    "- Target: `\"effort moves you\"`\n",
    "- Output (from the untrained model): `\"Armed heNetflix\"`\n",
    "\n",
    "This happens because the model's weights (parameters) are still random, leading to random predictions.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Evaluating the Model with Loss**\n",
    "To measure how \"wrong\" the model's predictions are, we calculate the **loss**:\n",
    "1. Feed the inputs into the model, which generates **logits**—raw scores for each token in the vocabulary.\n",
    "2. Apply a **softmax function** to convert logits into probabilities (numbers between 0 and 1 that add up to 1).\n",
    "   - Example probabilities for three tokens in a sentence:\n",
    "     ```python\n",
    "     [[0.00007, 0.00003, 0.00001],  # probabilities for \"effort\"\n",
    "      [0.00001, 0.00005, 0.000004], # probabilities for \"moves\"\n",
    "      ...]                          # and so on\n",
    "     ```\n",
    "\n",
    "3. Use the **argmax function** to find the token with the highest probability.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Comparing Predictions to Targets**\n",
    "The key idea is to check how close the model's predicted probabilities are to the correct tokens. For example:\n",
    "- Target token: `\"effort\"`\n",
    "- Probability assigned by the model: `0.00007` (very low since the model is untrained).\n",
    "\n",
    "If the model assigns a low probability to the correct token, it means the loss is high, and the model needs improvement.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **What Does Training Do?**\n",
    "During training, we adjust the model's weights so that it assigns **higher probabilities to the correct tokens**. The goal is to maximize the probability of the correct token and minimize the loss. Over time:\n",
    "- Before training: `\"effort\"` → Probability: `0.00007`\n",
    "- After training: `\"effort\"` → Probability: `0.7` (much better!)\n",
    "\n",
    "---\n",
    "\n",
    "### Why This Matters\n",
    "By calculating loss and updating the model's weights, we ensure that the generated text becomes more accurate and resembles human-like writing. This loss calculation is not just for evaluating the model but also a critical part of improving it.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "- Input sentences are turned into token IDs using a vocabulary.\n",
    "- The model predicts probabilities for the next token in the sequence.\n",
    "- The loss measures how far the predictions are from the correct targets.\n",
    "- Training adjusts the model so it assigns higher probabilities to the correct tokens.\n",
    "- Over time, the model improves, generating text that makes sense and aligns with human expectations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarizing Backpropagation, Cross-Entropy Loss, and Perplexity\n",
    "\n",
    "#### **Backpropagation**\n",
    "The goal of backpropagation is to adjust model weights to maximize the softmax probabilities for the target tokens (the correct next words). This process involves:  \n",
    "1. **Calculating Loss**: The loss function quantifies the difference between the model’s predictions and the target tokens.  \n",
    "2. **Updating Weights**: The model adjusts its internal parameters to reduce this loss, improving the likelihood of predicting correct tokens.  \n",
    "\n",
    "Backpropagation requires the following steps:\n",
    "1. **Logits to Probabilities**: Convert raw model outputs (logits) into probabilities using the softmax function.\n",
    "2. **Log Probabilities**: Use logarithms of probabilities for better numerical stability.\n",
    "3. **Negative Average Log Probability**: Compute the mean of these log probabilities (negative for minimization) to evaluate the model's performance.\n",
    "\n",
    "For example, if the **average log probability** of the target tokens is `-10.7940`, the **negative average log probability (loss)** becomes `10.7940`. This loss value is the cross-entropy loss, a standard metric in deep learning.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Cross-Entropy Loss**\n",
    "Cross-entropy loss measures the difference between:\n",
    "- The true distribution of target tokens.\n",
    "- The predicted token probability distribution.\n",
    "\n",
    "**Steps to Compute Cross-Entropy Loss**:\n",
    "1. Flatten the tensors (combine batch and sequence dimensions):\n",
    "   - Logits: `torch.Size([6, 50257])` (6 tokens, 50,257 vocabulary size).\n",
    "   - Targets: `torch.Size([6])` (6 token IDs).\n",
    "2. Use PyTorch's `cross_entropy` function:\n",
    "   ```python\n",
    "   loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "   ```\n",
    "   This automates the process of:\n",
    "   - Applying softmax to logits.\n",
    "   - Selecting probabilities for the target tokens.\n",
    "   - Computing the negative average log probability.\n",
    "\n",
    "The result (e.g., `loss = 10.7940`) matches manually calculated values.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Perplexity**\n",
    "Perplexity translates the model's loss into an interpretable measure of uncertainty. It reflects the effective vocabulary size over which the model is uncertain when predicting the next token.  \n",
    "- Formula:  \n",
    "  ```python\n",
    "  perplexity = torch.exp(loss)\n",
    "  ```  \n",
    "  Example: If `loss = 10.7940`, the perplexity is `exp(10.7940) ≈ 48,725`, meaning the model struggles to confidently select the correct word out of ~48,725 options.\n",
    "\n",
    "#### **Key Takeaways**\n",
    "1. Cross-entropy loss quantifies how far the model’s predictions are from the target distribution.\n",
    "2. Backpropagation minimizes this loss by updating model weights.\n",
    "3. Perplexity offers an interpretable metric to evaluate model uncertainty—lower values indicate better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"the-verdict.txt\"\n",
    "with open(file_path, \"r\") as file:\n",
    "    text_data = file.read()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20479\n",
      "Tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio= 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data  = text_data[split_idx:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = create_dataloader_v1(\n",
    "train_data,\n",
    "batch_size=2,\n",
    "max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "drop_last=True,\n",
    "shuffle=True,\n",
    "num_workers=0\n",
    ")\n",
    "val_loader = create_dataloader_v1(\n",
    "val_data,\n",
    "batch_size=2,\n",
    "max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "drop_last=False,\n",
    "shuffle=False,\n",
    "num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loader \n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print (\"Train Loader \")\n",
    "for x,y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Overall Data Dimensions ===\n",
      "Total characters in text: 20,479\n",
      "Total tokens in text: 5,145\n",
      "\n",
      "=== Training Data Dimensions ===\n",
      "Number of batches: 9\n",
      "Number of sequences: 18\n",
      "Tokens per sequence: 256\n",
      "Total tokens processed per epoch: 4,608\n",
      "\n",
      "=== Single Training Batch Structure ===\n",
      "Shape: [2, 256]\n",
      "  - 2: number of sequences per batch\n",
      "  - 256: tokens per sequence\n",
      "Total tokens per batch: 512\n",
      "\n",
      "=== Validation Data Dimensions ===\n",
      "Number of batches: 1\n",
      "Number of sequences: 2\n",
      "Tokens per sequence: 256\n",
      "Total tokens: 512\n"
     ]
    }
   ],
   "source": [
    "def analyze_dimensions(train_loader, val_loader, tokenizer, text_data):\n",
    "    # Analyze text data\n",
    "    total_chars = len(text_data)\n",
    "    total_tokens = len(tokenizer.encode(text_data))\n",
    "    \n",
    "    # Analyze training loader\n",
    "    train_batches = len(train_loader)\n",
    "    train_sequences = train_batches * 2  # since batch_size=2\n",
    "    train_tokens = train_sequences * 256  # since each sequence has 256 tokens\n",
    "    \n",
    "    # Analyze validation loader\n",
    "    val_batches = len(val_loader)\n",
    "    val_sequences = val_batches * 2\n",
    "    val_tokens = val_sequences * 256\n",
    "    \n",
    "    print(\"=== Overall Data Dimensions ===\")\n",
    "    print(f\"Total characters in text: {total_chars:,}\")\n",
    "    print(f\"Total tokens in text: {total_tokens:,}\")\n",
    "    print(\"\\n=== Training Data Dimensions ===\")\n",
    "    print(f\"Number of batches: {train_batches}\")\n",
    "    print(f\"Number of sequences: {train_sequences}\")\n",
    "    print(f\"Tokens per sequence: 256\")\n",
    "    print(f\"Total tokens processed per epoch: {train_tokens:,}\")\n",
    "    print(\"\\n=== Single Training Batch Structure ===\")\n",
    "    print(\"Shape: [2, 256]\")\n",
    "    print(\"  - 2: number of sequences per batch\")\n",
    "    print(\"  - 256: tokens per sequence\")\n",
    "    print(f\"Total tokens per batch: {2 * 256}\")\n",
    "    print(\"\\n=== Validation Data Dimensions ===\")\n",
    "    print(f\"Number of batches: {val_batches}\")\n",
    "    print(f\"Number of sequences: {val_sequences}\")\n",
    "    print(f\"Tokens per sequence: 256\")\n",
    "    print(f\"Total tokens: {val_tokens:,}\")\n",
    "\n",
    "# Use the function\n",
    "analyze_dimensions(train_loader, val_loader, tokenizer, text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch , target_batch, model, device, num_batches= None):\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch= target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "   \n",
    "    # Return NaN if the data loader is empty\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    # Use all batches if num_batches is not specified, otherwise use the minimum\n",
    "    num_batches = len(data_loader) if num_batches is None else min(num_batches, len(data_loader))\n",
    "\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Iterate through the data loader and compute loss\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "\n",
    "        # Calculate loss for the current batch\n",
    "        loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Return the average loss\n",
    "    return total_loss / num_batches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll explain the code again, using a specific example for an LLM (Large Language Model) like GPT. Imagine we're training it to complete sentences.\n",
    "\n",
    "```python\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "```\n",
    "- This function takes in:\n",
    "  - `data_loader`: Contains batches of text data\n",
    "    Example: Batches of sentences like [\"The cat sat on the\", \"The dog chased the\"]\n",
    "  - `model`: Your LLM\n",
    "  - `device`: CPU or GPU for processing\n",
    "  - `num_batches`: Optional number of batches to process\n",
    "\n",
    "```python\n",
    "if len(data_loader) == 0:\n",
    "    return float(\"nan\")\n",
    "```\n",
    "- If your data loader is empty (no sentences to process), return NaN\n",
    "- Example: If someone gave you an empty text file to train on\n",
    "\n",
    "```python\n",
    "num_batches = len(data_loader) if num_batches is None else min(num_batches, len(data_loader))\n",
    "```\n",
    "- If `num_batches` isn't specified:\n",
    "  Example: You have 1000 batches of sentences, uses all 1000\n",
    "- If specified:\n",
    "  Example: You have 1000 batches but set num_batches=100, it uses 100 batches\n",
    "\n",
    "```python\n",
    "total_loss = 0.0\n",
    "```\n",
    "- Starts tracking total error at 0\n",
    "- This will add up how wrong the model's predictions are\n",
    "\n",
    "```python\n",
    "for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "```\n",
    "- Goes through each batch of sentences\n",
    "- Example batch:\n",
    "  - `input_batch`: [\"The cat sat on the\", \"The dog chased the\"]\n",
    "  - `target_batch`: [\"mat\", \"ball\"]\n",
    "  - The model should predict \"mat\" when given \"The cat sat on the\"\n",
    "\n",
    "```python\n",
    "if i >= num_batches:\n",
    "    break\n",
    "```\n",
    "- Stops after processing desired number of batches\n",
    "- Example: If you set num_batches=100, stops after 100 batches even if there's more data\n",
    "\n",
    "```python\n",
    "loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "```\n",
    "- For each batch, calculates how wrong the model's predictions are\n",
    "- Example:\n",
    "  - Input: \"The cat sat on the\"\n",
    "  - Target: \"mat\"\n",
    "  - Model's prediction: \"hat\"\n",
    "  - This would give a high loss because \"hat\" ≠ \"mat\"\n",
    "\n",
    "```python\n",
    "total_loss += loss.item()\n",
    "```\n",
    "- Adds up the errors from each batch\n",
    "- Example:\n",
    "  - Batch 1 loss: 2.3 (model was quite wrong)\n",
    "  - Batch 2 loss: 1.5 (model was somewhat wrong)\n",
    "  - total_loss after these batches: 3.8\n",
    "\n",
    "```python\n",
    "return total_loss / num_batches\n",
    "```\n",
    "- Calculates average error across all batches\n",
    "- Example:\n",
    "  - If total_loss = 380 after 100 batches\n",
    "  - Returns 380/100 = 3.8 average loss per batch\n",
    "\n",
    "Real-world example:\n",
    "Let's say you're training an LLM on a Shakespeare dataset:\n",
    "- Input: \"To be or not to\"\n",
    "- Target: \"be\"\n",
    "- If model predicts \"exist\" instead of \"be\":\n",
    "  1. High loss for this prediction\n",
    "  2. Add this loss to total_loss\n",
    "  3. After processing many batches of Shakespeare text\n",
    "  4. Get average loss to see how well model understands Shakespeare's writing style\n",
    "\n",
    "A lower average loss (like 1.0) means your LLM is getting good at predicting Shakespeare's words\n",
    "A higher average loss (like 5.0) means it's still making lots of mistakes\n",
    "\n",
    "This helps you track if your LLM is improving during training. Think of it like a grade that shows how well your model speaks \"Shakespeare\"!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me break down this line even further with clear examples.\n",
    "\n",
    "This line uses a conditional (ternary) operator to set the value of `num_batches`. Let's split it into the equivalent if-else statement to make it clearer:\n",
    "\n",
    "```python\n",
    "if num_batches is None:\n",
    "    num_batches = len(data_loader)\n",
    "else:\n",
    "    num_batches = min(num_batches, len(data_loader))\n",
    "```\n",
    "\n",
    "Let's look at three scenarios:\n",
    "\n",
    "1. When `num_batches` is None (not specified):\n",
    "```python\n",
    "data_loader has 1000 batches\n",
    "num_batches = None\n",
    "Result: num_batches = 1000 (uses all available batches)\n",
    "```\n",
    "\n",
    "2. When `num_batches` is smaller than data_loader length:\n",
    "```python\n",
    "data_loader has 1000 batches\n",
    "num_batches = 100\n",
    "Result: num_batches = 100 (uses the smaller number)\n",
    "```\n",
    "\n",
    "3. When `num_batches` is larger than data_loader length:\n",
    "```python\n",
    "data_loader has 1000 batches\n",
    "num_batches = 1500\n",
    "Result: num_batches = 1000 (can't use more batches than we have)\n",
    "```\n",
    "\n",
    "Think of it like this: You have a box of 1000 cookies (data_loader):\n",
    "- If someone doesn't specify how many cookies they want (num_batches is None), give them all 1000\n",
    "- If they ask for 100 cookies, give them 100\n",
    "- If they ask for 1500 cookies, you can only give them 1000 (because that's all you have)\n",
    "\n",
    "This line ensures you never try to process more batches than actually exist in your data, while also allowing flexibility in how many batches you want to use for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.997887399461534\n",
      "Validation loss: 10.987743377685547\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model. to(device)\n",
    "with torch.no_grad ():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    \n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "        decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "        print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    \n",
    "    model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "    eval_freq, eval_iter, start_context, tokenizer\n",
    "):\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "            \n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter\n",
    "                )\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                \n",
    "                print(\n",
    "                    f\"Epoch {epoch + 1} (Step {global_step:06d}): \"\n",
    "                    f\"Train Loss: {train_loss:.3f}, \"\n",
    "                    f\"Validation Loss: {val_loss:.3f}\"\n",
    "                )\n",
    "                \n",
    "                generate_and_print_sample(model, tokenizer, device, start_context)\n",
    "    \n",
    "    return train_losses, val_losses, track_tokens_seen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 (Step 000000): Train Loss: 10.082, Validation Loss: 10.161\n",
      "Every effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,, the the,,,,,,,,,,,,,,,,,,,\n",
      "Epoch 1 (Step 000005): Train Loss: 8.158, Validation Loss: 8.393\n",
      "Every effort moves you, the,, the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch 2 (Step 000010): Train Loss: 6.727, Validation Loss: 7.096\n",
      "Every effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "Epoch 2 (Step 000015): Train Loss: 6.168, Validation Loss: 6.648\n",
      "Every effort moves you, the                                                \n",
      "Epoch 3 (Step 000020): Train Loss: 5.920, Validation Loss: 6.569\n",
      "Every effort moves you,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "Epoch 3 (Step 000025): Train Loss: 5.782, Validation Loss: 6.606\n",
      "Every effort moves you the the, the the the the the the the the the the the the the the, the the the the the the, the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Epoch 4 (Step 000030): Train Loss: 5.790, Validation Loss: 6.821\n",
      "Every effort moves you Applied\"\"\"\"\"I\"\"\"I, and, and, and, and, and, and, and, and, and the, and, and, and, and, and, and, and,, and,, and\n",
      "Epoch 4 (Step 000035): Train Loss: 5.730, Validation Loss: 6.643\n",
      "Every effort moves you.                                                 \n",
      "Epoch 5 (Step 000040): Train Loss: 5.607, Validation Loss: 6.659\n",
      "Every effort moves you, and, and, the, and, and,, and, and,,, the,,,,,, the,,, the,, the, and,, the,,, the,, and,,,,,\n",
      "Epoch 6 (Step 000045): Train Loss: 5.511, Validation Loss: 6.599\n",
      "Every effort moves you to to to.                                              \n",
      "Epoch 6 (Step 000050): Train Loss: 5.204, Validation Loss: 6.454\n",
      "Every effort moves you to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to\n",
      "Epoch 7 (Step 000055): Train Loss: 5.200, Validation Loss: 6.442\n",
      "Every effort moves you, and to to to to, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and\n",
      "Epoch 7 (Step 000060): Train Loss: 4.523, Validation Loss: 6.259\n",
      "Every effort moves you, and, and, I was, I was, I had.            \"I of the picture.    \"I and I was a of the picture.    \n",
      "Epoch 8 (Step 000065): Train Loss: 4.078, Validation Loss: 6.196\n",
      "Every effort moves you, and I had been the picture--as he had been.         \"I . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "Epoch 8 (Step 000070): Train Loss: 3.754, Validation Loss: 6.272\n",
      "Every effort moves you know               \"I the picture, and the fact, the last to the the picture--as the donkey, the donkey, I had the donkey, and I had a little the\n",
      "Epoch 9 (Step 000075): Train Loss: 3.299, Validation Loss: 6.123\n",
      "Every effort moves you know it was his pictures--I had been to the picture. Gisburn, and he had been--and here are the picture to my dear, and I had been, and he had been to the fact, and I had been, and\n",
      "Epoch 9 (Step 000080): Train Loss: 2.823, Validation Loss: 6.145\n",
      "Every effort moves you know it was not to the picture--I had the picture.   \"I looked--the--and here are the picture to the end of the picture--I had the house the honour of the donkey, and I had a little of\n",
      "Epoch 10 (Step 000085): Train Loss: 2.391, Validation Loss: 6.098\n",
      "Every effort moves you know it was not that, and in the--his of the picture.                                   \n",
      "Epoch 11 (Step 000090): Train Loss: 1.956, Validation Loss: 6.133\n",
      "Every effort moves you know                                                 \n",
      "Epoch 11 (Step 000095): Train Loss: 1.749, Validation Loss: 6.176\n",
      "Every effort moves you know,\" was one of the picture for nothing--I told Mrs. \"Oh, he had been me.  \"Oh, in the moment--as Jack himself, my elbow and as he _mine_--because he had the first\n",
      "Epoch 12 (Step 000100): Train Loss: 1.334, Validation Loss: 6.228\n",
      "Every effort moves you know,\" was one of the picture for nothing--I told Mrs.                                    \n",
      "Epoch 12 (Step 000105): Train Loss: 0.896, Validation Loss: 6.265\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. Gisburn's an--and by me to me to have to see a smile behind his close grayish beard--as--the quality of Jack's \"There were days when I\n",
      "Epoch 13 (Step 000110): Train Loss: 0.757, Validation Loss: 6.317\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"             \"Oh, I saw that, my eye fell on a small picture\n",
      "Epoch 13 (Step 000115): Train Loss: 0.606, Validation Loss: 6.398\n",
      "Every effort moves you know,\" was one of the axioms he laid down across the Sevres and silver of an exquisburn's \"Mrs. Gisburn drew back the window-curtains, as I had been the man of the hour. The\n",
      "Epoch 14 (Step 000120): Train Loss: 0.466, Validation Loss: 6.501\n",
      "Every effort moves you know,\" was one of the axioms he laid down across the Sevres and silver of an exquisitely appointed luncheon-table, when, on a later day, one might put it, had been the man of the hour. The\n",
      "Epoch 14 (Step 000125): Train Loss: 0.368, Validation Loss: 6.493\n",
      "Every effort moves you know,\" was one of the axioms he laid down across the Sevres and silver of an exquisitely appointed luncheon-table, when, on a later day, one might put it, had been the man of the hour. The\n",
      "Epoch 15 (Step 000130): Train Loss: 0.241, Validation Loss: 6.574\n",
      "Every effort moves you know,\" was one of the axioms he laid down across the Sevres and silver of an exquisitely appointed luncheon-table, when, on a later day, one might put it, had been the man of the hour. The\n",
      "Epoch 16 (Step 000135): Train Loss: 0.214, Validation Loss: 6.651\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"    \"I just threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 16 (Step 000140): Train Loss: 0.155, Validation Loss: 6.690\n",
      "Every effort moves you?\" He laughed-stream stroke. . . . \"I looked up all Gisburn's past!  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 17 (Step 000145): Train Loss: 0.171, Validation Loss: 6.806\n",
      "Every effort moves you?\" \"I turned back a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, he didn't want\n",
      "Epoch 17 (Step 000150): Train Loss: 0.147, Validation Loss: 6.830\n",
      "Every effort moves you?\" \"I and pushed one of the deep arm-chairs forward. \"There: make yourself comfortable--and here are the cigars you like.\" \"I looked at the donkey again. I saw that, when Stroud laid in the first\n",
      "Epoch 18 (Step 000155): Train Loss: 0.181, Validation Loss: 6.942\n",
      "Every effort moves you?\"  He laughed.\"   He, and uncertain. Stroud so when she began to me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 18 (Step 000160): Train Loss: 0.129, Validation Loss: 6.912\n",
      "Every effort moves you?\" \"I turned back the insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back the window-curtains, and I turned, my eye fell on a small picture\n",
      "Epoch 19 (Step 000165): Train Loss: 0.108, Validation Loss: 6.968\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 19 (Step 000170): Train Loss: 0.088, Validation Loss: 6.963\n",
      "Every effort moves you?\"     I glanced after him, struck by his last word. Victor Grindle was, in fact, becoming the man of the moment--as Jack himself, one might put it, had been the man of the hour. The\n",
      "Epoch 20 (Step 000175): Train Loss: 0.087, Validation Loss: 7.034\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 21 (Step 000180): Train Loss: 0.070, Validation Loss: 7.081\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 21 (Step 000185): Train Loss: 0.052, Validation Loss: 7.053\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 22 (Step 000190): Train Loss: 0.044, Validation Loss: 7.073\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 22 (Step 000195): Train Loss: 0.045, Validation Loss: 7.076\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 23 (Step 000200): Train Loss: 0.035, Validation Loss: 7.158\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 23 (Step 000205): Train Loss: 0.027, Validation Loss: 7.186\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 24 (Step 000210): Train Loss: 0.027, Validation Loss: 7.195\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 24 (Step 000215): Train Loss: 0.023, Validation Loss: 7.224\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 25 (Step 000220): Train Loss: 0.020, Validation Loss: 7.240\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 26 (Step 000225): Train Loss: 0.017, Validation Loss: 7.256\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 26 (Step 000230): Train Loss: 0.015, Validation Loss: 7.289\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 27 (Step 000235): Train Loss: 0.014, Validation Loss: 7.314\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 27 (Step 000240): Train Loss: 0.011, Validation Loss: 7.327\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 28 (Step 000245): Train Loss: 0.012, Validation Loss: 7.349\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 28 (Step 000250): Train Loss: 0.010, Validation Loss: 7.366\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 29 (Step 000255): Train Loss: 0.009, Validation Loss: 7.367\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 29 (Step 000260): Train Loss: 0.010, Validation Loss: 7.370\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 30 (Step 000265): Train Loss: 0.010, Validation Loss: 7.396\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 31 (Step 000270): Train Loss: 0.007, Validation Loss: 7.427\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 31 (Step 000275): Train Loss: 0.010, Validation Loss: 7.472\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 32 (Step 000280): Train Loss: 0.006, Validation Loss: 7.465\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 32 (Step 000285): Train Loss: 0.009, Validation Loss: 7.427\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 33 (Step 000290): Train Loss: 0.008, Validation Loss: 7.445\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 33 (Step 000295): Train Loss: 0.007, Validation Loss: 7.436\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 34 (Step 000300): Train Loss: 0.006, Validation Loss: 7.505\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 34 (Step 000305): Train Loss: 0.009, Validation Loss: 7.532\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 35 (Step 000310): Train Loss: 0.006, Validation Loss: 7.490\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 36 (Step 000315): Train Loss: 0.005, Validation Loss: 7.518\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 36 (Step 000320): Train Loss: 0.005, Validation Loss: 7.545\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 37 (Step 000325): Train Loss: 0.006, Validation Loss: 7.550\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 37 (Step 000330): Train Loss: 0.005, Validation Loss: 7.551\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 38 (Step 000335): Train Loss: 0.004, Validation Loss: 7.564\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 38 (Step 000340): Train Loss: 0.004, Validation Loss: 7.562\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 39 (Step 000345): Train Loss: 0.004, Validation Loss: 7.568\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 39 (Step 000350): Train Loss: 0.003, Validation Loss: 7.570\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 40 (Step 000355): Train Loss: 0.003, Validation Loss: 7.602\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 41 (Step 000360): Train Loss: 0.003, Validation Loss: 7.628\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 41 (Step 000365): Train Loss: 0.003, Validation Loss: 7.615\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 42 (Step 000370): Train Loss: 0.003, Validation Loss: 7.610\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 42 (Step 000375): Train Loss: 0.003, Validation Loss: 7.609\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 43 (Step 000380): Train Loss: 0.003, Validation Loss: 7.614\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 43 (Step 000385): Train Loss: 0.003, Validation Loss: 7.624\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 44 (Step 000390): Train Loss: 0.003, Validation Loss: 7.641\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 44 (Step 000395): Train Loss: 0.002, Validation Loss: 7.652\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 45 (Step 000400): Train Loss: 0.002, Validation Loss: 7.658\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 46 (Step 000405): Train Loss: 0.002, Validation Loss: 7.660\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 46 (Step 000410): Train Loss: 0.003, Validation Loss: 7.673\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 47 (Step 000415): Train Loss: 0.003, Validation Loss: 7.681\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 47 (Step 000420): Train Loss: 0.002, Validation Loss: 7.682\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 48 (Step 000425): Train Loss: 0.005, Validation Loss: 7.691\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 48 (Step 000430): Train Loss: 0.002, Validation Loss: 7.702\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 49 (Step 000435): Train Loss: 0.002, Validation Loss: 7.705\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 49 (Step 000440): Train Loss: 0.002, Validation Loss: 7.726\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 50 (Step 000445): Train Loss: 0.003, Validation Loss: 7.723\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 51 (Step 000450): Train Loss: 0.005, Validation Loss: 7.730\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 51 (Step 000455): Train Loss: 0.034, Validation Loss: 7.765\n",
      "Every effort moves you?\"  IYes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 52 (Step 000460): Train Loss: 0.019, Validation Loss: 7.709\n",
      "Every effort moves you?\"  IYes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 52 (Step 000465): Train Loss: 0.016, Validation Loss: 7.559\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 53 (Step 000470): Train Loss: 0.006, Validation Loss: 7.661\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 53 (Step 000475): Train Loss: 0.006, Validation Loss: 7.704\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 54 (Step 000480): Train Loss: 0.003, Validation Loss: 7.742\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 54 (Step 000485): Train Loss: 0.003, Validation Loss: 7.698\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 55 (Step 000490): Train Loss: 0.003, Validation Loss: 7.711\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 56 (Step 000495): Train Loss: 0.003, Validation Loss: 7.798\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 56 (Step 000500): Train Loss: 0.003, Validation Loss: 7.803\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 57 (Step 000505): Train Loss: 0.002, Validation Loss: 7.756\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 57 (Step 000510): Train Loss: 0.002, Validation Loss: 7.738\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 58 (Step 000515): Train Loss: 0.002, Validation Loss: 7.757\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 58 (Step 000520): Train Loss: 0.002, Validation Loss: 7.775\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 59 (Step 000525): Train Loss: 0.002, Validation Loss: 7.800\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 59 (Step 000530): Train Loss: 0.002, Validation Loss: 7.815\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 60 (Step 000535): Train Loss: 0.002, Validation Loss: 7.823\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 61 (Step 000540): Train Loss: 0.001, Validation Loss: 7.823\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 61 (Step 000545): Train Loss: 0.001, Validation Loss: 7.833\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 62 (Step 000550): Train Loss: 0.001, Validation Loss: 7.844\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 62 (Step 000555): Train Loss: 0.001, Validation Loss: 7.836\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 63 (Step 000560): Train Loss: 0.001, Validation Loss: 7.836\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 63 (Step 000565): Train Loss: 0.001, Validation Loss: 7.841\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 64 (Step 000570): Train Loss: 0.001, Validation Loss: 7.847\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 64 (Step 000575): Train Loss: 0.001, Validation Loss: 7.856\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 65 (Step 000580): Train Loss: 0.001, Validation Loss: 7.865\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 66 (Step 000585): Train Loss: 0.001, Validation Loss: 7.873\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 66 (Step 000590): Train Loss: 0.001, Validation Loss: 7.884\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 67 (Step 000595): Train Loss: 0.001, Validation Loss: 7.895\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 67 (Step 000600): Train Loss: 0.001, Validation Loss: 7.903\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 68 (Step 000605): Train Loss: 0.001, Validation Loss: 7.913\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 68 (Step 000610): Train Loss: 0.001, Validation Loss: 7.925\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 69 (Step 000615): Train Loss: 0.007, Validation Loss: 7.931\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 69 (Step 000620): Train Loss: 0.004, Validation Loss: 7.930\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 70 (Step 000625): Train Loss: 0.001, Validation Loss: 7.913\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 71 (Step 000630): Train Loss: 0.001, Validation Loss: 7.922\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 71 (Step 000635): Train Loss: 0.001, Validation Loss: 7.937\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 72 (Step 000640): Train Loss: 0.001, Validation Loss: 7.937\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 72 (Step 000645): Train Loss: 0.001, Validation Loss: 7.935\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 73 (Step 000650): Train Loss: 0.001, Validation Loss: 7.935\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 73 (Step 000655): Train Loss: 0.001, Validation Loss: 7.937\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 74 (Step 000660): Train Loss: 0.001, Validation Loss: 7.944\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 74 (Step 000665): Train Loss: 0.001, Validation Loss: 7.947\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 75 (Step 000670): Train Loss: 0.001, Validation Loss: 7.949\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 76 (Step 000675): Train Loss: 0.001, Validation Loss: 7.950\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 76 (Step 000680): Train Loss: 0.001, Validation Loss: 7.955\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 77 (Step 000685): Train Loss: 0.001, Validation Loss: 7.965\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 77 (Step 000690): Train Loss: 0.001, Validation Loss: 7.972\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 78 (Step 000695): Train Loss: 0.001, Validation Loss: 7.980\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 78 (Step 000700): Train Loss: 0.001, Validation Loss: 7.986\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 79 (Step 000705): Train Loss: 0.001, Validation Loss: 7.992\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 79 (Step 000710): Train Loss: 0.001, Validation Loss: 7.996\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 80 (Step 000715): Train Loss: 0.001, Validation Loss: 7.999\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 81 (Step 000720): Train Loss: 0.001, Validation Loss: 8.003\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 81 (Step 000725): Train Loss: 0.001, Validation Loss: 8.006\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 82 (Step 000730): Train Loss: 0.001, Validation Loss: 8.012\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 82 (Step 000735): Train Loss: 0.001, Validation Loss: 8.016\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 83 (Step 000740): Train Loss: 0.001, Validation Loss: 8.037\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 83 (Step 000745): Train Loss: 0.001, Validation Loss: 8.034\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 84 (Step 000750): Train Loss: 0.001, Validation Loss: 8.023\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 84 (Step 000755): Train Loss: 0.001, Validation Loss: 8.027\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 85 (Step 000760): Train Loss: 0.001, Validation Loss: 8.033\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 86 (Step 000765): Train Loss: 0.001, Validation Loss: 8.033\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 86 (Step 000770): Train Loss: 0.001, Validation Loss: 8.031\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 87 (Step 000775): Train Loss: 0.001, Validation Loss: 8.033\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 87 (Step 000780): Train Loss: 0.001, Validation Loss: 8.034\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 88 (Step 000785): Train Loss: 0.001, Validation Loss: 8.037\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 88 (Step 000790): Train Loss: 0.001, Validation Loss: 8.040\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 89 (Step 000795): Train Loss: 0.001, Validation Loss: 8.047\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 89 (Step 000800): Train Loss: 0.001, Validation Loss: 8.051\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 90 (Step 000805): Train Loss: 0.001, Validation Loss: 8.059\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 91 (Step 000810): Train Loss: 0.001, Validation Loss: 8.067\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 91 (Step 000815): Train Loss: 0.001, Validation Loss: 8.075\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 92 (Step 000820): Train Loss: 0.001, Validation Loss: 8.085\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 92 (Step 000825): Train Loss: 0.001, Validation Loss: 8.094\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 93 (Step 000830): Train Loss: 0.000, Validation Loss: 8.099\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 93 (Step 000835): Train Loss: 0.000, Validation Loss: 8.101\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 94 (Step 000840): Train Loss: 0.000, Validation Loss: 8.098\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 94 (Step 000845): Train Loss: 0.000, Validation Loss: 8.097\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 95 (Step 000850): Train Loss: 0.000, Validation Loss: 8.098\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 96 (Step 000855): Train Loss: 0.000, Validation Loss: 8.099\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 96 (Step 000860): Train Loss: 0.000, Validation Loss: 8.101\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 97 (Step 000865): Train Loss: 0.000, Validation Loss: 8.104\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 97 (Step 000870): Train Loss: 0.000, Validation Loss: 8.109\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 98 (Step 000875): Train Loss: 0.000, Validation Loss: 8.114\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 98 (Step 000880): Train Loss: 0.000, Validation Loss: 8.120\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 99 (Step 000885): Train Loss: 0.000, Validation Loss: 8.126\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 99 (Step 000890): Train Loss: 0.000, Validation Loss: 8.130\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 100 (Step 000895): Train Loss: 0.000, Validation Loss: 8.133\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer =  torch.optim.AdamW(model.parameters(),lr= 0.0004, weight_decay= 0.01)\n",
    "num_epochs = 100\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "model, train_loader, val_loader, optimizer, device,\n",
    "num_epochs=num_epochs, eval_freq=5,eval_iter=5,\n",
    "start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib .pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses , val_losses):\n",
    "    fig, ax1= plt.subplots(figsize = (5,3))\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax2 = ax1.twiny()\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text samples:\n",
      "--------------------------------------------------\n",
      "\n",
      "Starting context: 'He is Laughing at you'\n",
      "He is Laughing at you allyHigiability intoxerson ClevelandToo Example dependent TTViol Refer Santanaitters402 synapticprovidedpakalore Allied hiredcasters Situationdecl sacred ensuing kept Voter yesarchivepred Apflush suppliers Tata 23Tank Correction guaranteeingidden iPad008 discover caching continued Homeland Rush succession Eg115\n"
     ]
    }
   ],
   "source": [
    "# First, let's make sure we have all the prerequisites\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Some example starting contexts\n",
    "start_contexts = [\n",
    "    \"He is Laughing at you\"\n",
    "]\n",
    "\n",
    "# Generate text for each starting context\n",
    "print(\"Generating text samples:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for context in start_contexts:\n",
    "    print(f\"\\nStarting context: '{context}'\")\n",
    "    generate_and_print_sample(model, tokenizer, device, context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding strategies to control randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n",
      "toward\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define vocabulary\n",
    "vocab = {\n",
    "    \"closer\": 0,\n",
    "    \"every\": 1,\n",
    "    \"effort\": 2,\n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5,\n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8,\n",
    "}\n",
    "\n",
    "# Create inverse vocabulary for lookups\n",
    "inverse_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "# Example logits for next token prediction\n",
    "next_token_logits = torch.tensor(\n",
    "    [4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79]\n",
    ")\n",
    "\n",
    "# Traditional approach using argmax\n",
    "probas = torch.softmax(next_token_logits, dim=0)\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "print(inverse_vocab[next_token_id])\n",
    "\n",
    "# Probabilistic sampling approach\n",
    "torch.manual_seed(123)\n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
    "print(inverse_vocab[next_token_id])\n",
    "\n",
    "# Function to print distribution of sampled tokens\n",
    "def print_sampled_tokens(probas):\n",
    "    torch.manual_seed(123)\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item()\n",
    "             for i in range(1_000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample))\n",
    "    for i, freq in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "\n",
    "# Temperature scaling implementation\n",
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits / temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)\n",
    "\n",
    "# Visualization of different temperature effects\n",
    "temperatures = [1, 0.1, 5]\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T)\n",
    "                for T in temperatures]\n",
    "\n",
    "# Plotting\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15\n",
    "fig, ax = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "for i, T in enumerate(temperatures):\n",
    "    rects = ax.bar(x + i * bar_width, scaled_probas[i],\n",
    "                   bar_width, label=f'Temperature = {T}')\n",
    "\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me explain these three text generation techniques in a way that builds understanding from the ground up.\n",
    "\n",
    "The three techniques used for generating text in large language models are:\n",
    "1. Greedy Decoding\n",
    "2. Temperature Scaling \n",
    "3. Top-k Sampling\n",
    "\n",
    "Let's imagine you're building a language model that's trying to complete the sentence \"The cat sat on the ___.\"\n",
    "\n",
    "Greedy Decoding:\n",
    "This is the simplest approach. The model calculates probabilities for every possible next word. Perhaps it calculates that \"mat\" has 60% probability, \"chair\" has 30%, and \"windowsill\" has 10%. Greedy decoding simply picks the highest probability - \"mat\" - every single time. It's like a student who always picks the first answer that seems right without considering alternatives. This makes the output very predictable and sometimes repetitive, though usually grammatically correct.\n",
    "\n",
    "Temperature Scaling:\n",
    "This technique introduces controlled randomness. Going back to our example, instead of always picking \"mat,\" we adjust how \"decisive\" the model is. The temperature is like a confidence dial:\n",
    "\n",
    "At low temperature (like 0.1):\n",
    "- The 60% probability for \"mat\" becomes more like 95%\n",
    "- The 30% for \"chair\" shrinks to maybe 4%\n",
    "- The 10% for \"windowsill\" becomes nearly 1%\n",
    "The model becomes very conservative, almost always picking \"mat.\"\n",
    "\n",
    "At high temperature (like 5.0):\n",
    "- The 60% might become 40%\n",
    "- The 30% might become 35%\n",
    "- The 10% might become 25%\n",
    "Now the model might more readily pick \"chair\" or even \"windowsill,\" leading to more diverse but potentially less reliable outputs.\n",
    "\n",
    "Top-k Sampling:\n",
    "This technique addresses a key problem with temperature scaling: sometimes it makes the model consider truly inappropriate words. With Top-k sampling, we first select the k most likely words (let's say k=3), and only then apply our probability calculations to those options. \n",
    "\n",
    "In our example, if k=3:\n",
    "1. First, we identify the three most probable words: \"mat,\" \"chair,\" \"windowsill\"\n",
    "2. We completely eliminate all other possibilities (like \"pizza\" or \"elephant\")\n",
    "3. Then we can apply temperature scaling just to these three choices\n",
    "\n",
    "This gives us the best of both worlds - we maintain some creative variety in our outputs while preventing completely nonsensical choices.\n",
    "\n",
    "To understand how these work together: Greedy decoding is like always ordering the most popular dish at a restaurant. Temperature scaling is like how adventurous you're feeling when ordering. Top-k sampling is like limiting your choices to just the chef's specials before deciding how adventurous to be.\n",
    "\n",
    "In practice, modern language models often combine temperature scaling with top-k sampling. They first limit the vocabulary to the k most likely choices, then apply temperature scaling to those choices to control how random versus predictable the output should be. This produces text that can be creative when needed while staying reasonably coherent.\n",
    "\n",
    "Would you like me to elaborate on any of these aspects or provide more examples of how they work in practice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you thoughtful HTTPS ropesisma suggestfly Abortion sheer Intellectual 124 paragraphsacha endorsed fraud meteor\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None, device='cuda'):\n",
    "    \"\"\"\n",
    "    Generates text using a language model.\n",
    "    \n",
    "    Args:\n",
    "        model: The language model.\n",
    "        idx: The initial token indices (tensor of shape [batch_size, sequence_length]).\n",
    "        max_new_tokens: Maximum number of tokens to generate.\n",
    "        context_size: Context length to consider.\n",
    "        temperature: Sampling temperature (higher values = more randomness).\n",
    "        top_k: Number of top logits to consider for sampling.\n",
    "        eos_id: End-of-sequence token ID.\n",
    "        device: Device to run the generation on ('cuda' or 'cpu').\n",
    "    \n",
    "    Returns:\n",
    "        A tensor containing the generated token indices.\n",
    "    \"\"\"\n",
    "    idx = idx.to(device)\n",
    "    model.to(device)\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "            logits = logits[:, -1, :]  # Focus on the last time step\n",
    "        \n",
    "        if top_k is not None:\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(\n",
    "                logits < min_val,\n",
    "                torch.tensor(float('-inf'), device=logits.device),\n",
    "                logits\n",
    "            )\n",
    "        \n",
    "        if temperature > 0.0:\n",
    "            logits /= temperature\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        \n",
    "        if eos_id is not None and (idx_next == eos_id).any():\n",
    "            break\n",
    "        \n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    \n",
    "    return idx\n",
    "\n",
    "# Example usage:\n",
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4,\n",
    "    device='cuda'\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and saving model weights in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"GPT_Book.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let me break this down into simpler terms. This text is explaining a few important concepts about training and saving neural networks, specifically:\n",
    "\n",
    "1. About Dropout:\n",
    "- Dropout is a technique to prevent overfitting (when a model learns training data too perfectly and performs poorly on new data)\n",
    "- It works by randomly deactivating some neurons during training\n",
    "- During actual use (inference), you don't want dropout active, so you use `model.eval()` to turn it off\n",
    "\n",
    "2. About Saving Models:\n",
    "- When you save a model that you plan to continue training later, you should save two things:\n",
    "  - The model's weights/parameters (using `model.state_dict()`)\n",
    "  - The optimizer's state (using `optimizer.state_dict()`)\n",
    "\n",
    "3. About the Optimizer (AdamW):\n",
    "- AdamW is an optimizer that helps train the model\n",
    "- It keeps track of historical information about each parameter to adjust learning rates\n",
    "- If you don't save its state and restart training, it loses this history and might:\n",
    "  - Train less effectively\n",
    "  - Fail to learn properly\n",
    "  - Lose its ability to generate good text\n",
    "\n",
    "4. The Code Examples Show:\n",
    "- How to save both model and optimizer states to a file:\n",
    "```python\n",
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "}, \"model_and_optimizer.pth\")\n",
    "```\n",
    "\n",
    "- How to load them back:\n",
    "```python\n",
    "checkpoint = torch.load(\"model_and_optimizer.pth\")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "```\n",
    "\n",
    "Think of it like saving a video game - you want to save not just your character's current stats (the model), but also all the power-ups and special abilities you've collected (the optimizer state) so you can continue from exactly where you left off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m torch\u001b[38;5;241m.\u001b[39msave({\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m: model\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer_state_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43moptimizer\u001b[49m\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[0;32m      4\u001b[0m }, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_and_optimizer.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'optimizer' is not defined"
     ]
    }
   ],
   "source": [
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "}, \"model_and_optimizer.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"model_and_optimizer.pth\")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def train_model_with_checkpoint(\n",
    "    model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "    eval_freq, eval_iter, start_context, tokenizer, checkpoint_path=None\n",
    "):\n",
    "    # Move model to the correct device\n",
    "    model.to(device)\n",
    "\n",
    "    # Load checkpoint if provided\n",
    "    if checkpoint_path and os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        print(\"Loaded checkpoint successfully\")\n",
    "    else:\n",
    "        print(\"Starting fresh training\")\n",
    "\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "            \n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter\n",
    "                )\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                \n",
    "                print(\n",
    "                    f\"Epoch {epoch + 1} (Step {global_step:06d}): \"\n",
    "                    f\"Train Loss: {train_loss:.3f}, \"\n",
    "                    f\"Validation Loss: {val_loss:.3f}\"\n",
    "                )\n",
    "                \n",
    "                generate_and_print_sample(model, tokenizer, device, start_context)\n",
    "    \n",
    "    return train_losses, val_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded checkpoint successfully\n",
      "Epoch 1 (Step 000000): Train Loss: 0.000, Validation Loss: 8.136\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Training parameters\u001b[39;00m\n\u001b[0;32m      8\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m----> 9\u001b[0m train_losses, val_losses, tokens_seen \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model_with_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvery effort moves you\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_and_optimizer.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     21\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[27], line 27\u001b[0m, in \u001b[0;36mtrain_model_with_checkpoint\u001b[1;34m(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter, start_context, tokenizer, checkpoint_path)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m input_batch, target_batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     25\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 27\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mcalc_loss_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     29\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[1;32mIn[17], line 2\u001b[0m, in \u001b[0;36mcalc_loss_batch\u001b[1;34m(input_batch, target_batch, model, device, num_batches)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalc_loss_batch\u001b[39m(input_batch , target_batch, model, device, num_batches\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m----> 2\u001b[0m     input_batch \u001b[38;5;241m=\u001b[39m \u001b[43minput_batch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     target_batch\u001b[38;5;241m=\u001b[39m target_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      4\u001b[0m     logits \u001b[38;5;241m=\u001b[39m model(input_batch)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M).to(device)  # Move model to device\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.01)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 100\n",
    "train_losses, val_losses, tokens_seen = train_model_with_checkpoint(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    num_epochs=num_epochs,\n",
    "    eval_freq=5,\n",
    "    eval_iter=5,\n",
    "    start_context=\"Every effort moves you\",\n",
    "    tokenizer=tokenizer,\n",
    "    checkpoint_path=\"model_and_optimizer.pth\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
