{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of the previous chapter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First batch: [tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n",
      "Second batch: [tensor([[2885, 1464, 1807, 3619]]), tensor([[1464, 1807, 3619,  402]])]\n",
      "Token Embeddings Shape: torch.Size([1, 4, 256])\n",
      "Positional Embeddings Shape: torch.Size([1, 4, 256])\n",
      "Final Input Embeddings Shape: torch.Size([1, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import torch\n",
    "import tiktoken  \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Download the text file\n",
    "url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "       \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "       \"the-verdict.txt\")\n",
    "file_path = \"the-verdict.txt\"\n",
    "urllib.request.urlretrieve(url, file_path)\n",
    "\n",
    "# Load and tokenize the text\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")  \n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "\n",
    "# Define a custom dataset class\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "# Function to create a DataLoader\n",
    "def create_dataloader_v1(txt, batch_size, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "    return dataloader \n",
    "\n",
    "# Create DataLoader\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=1, max_length=4, stride=2, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "second_batch = next(data_iter)\n",
    "print(\"First batch:\", first_batch)\n",
    "print(\"Second batch:\", second_batch)\n",
    "\n",
    "# --- Adding Token and Positional Embeddings ---\n",
    "\n",
    "# Define token embedding layer\n",
    "vocab_size = 50257  # Typical size for GPT models\n",
    "embedding_dim = 256  # Example embedding size (GPT-3 uses 12,288)\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "# Convert token IDs into token embeddings\n",
    "token_embeddings = token_embedding_layer(first_batch[0])  # First batch of inputs\n",
    "print(\"Token Embeddings Shape:\", token_embeddings.shape)\n",
    "\n",
    "# Define positional embedding layer\n",
    "max_length = 4  # Same as the max sequence length\n",
    "pos_embedding_layer = torch.nn.Embedding(max_length, embedding_dim)\n",
    "\n",
    "# Generate position embeddings\n",
    "positions = torch.arange(max_length).unsqueeze(0)  # Create position indices\n",
    "pos_embeddings = pos_embedding_layer(positions)\n",
    "print(\"Positional Embeddings Shape:\", pos_embeddings.shape)\n",
    "\n",
    "# Combine token and positional embeddings\n",
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(\"Final Input Embeddings Shape:\", input_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch 3 - Coding Attention mechanisms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This passage explains the self-attention mechanism in large language models (LLMs), a crucial part of transformer-based architectures like GPT. Here’s a summary of the key concepts and steps covered:  \n",
    "\n",
    "### **1. The Problem with Long Sequences**  \n",
    "- Traditional models like **Recurrent Neural Networks (RNNs)** process text sequentially and struggle with long-range dependencies.  \n",
    "- The **encoder-decoder RNN architecture** attempts to capture context but compresses all information into a single hidden state, leading to loss of information over long sequences.  \n",
    "- **Attention mechanisms** were introduced to solve this by allowing the model to focus on different parts of the input while generating output.  \n",
    "\n",
    "### **2. Introduction to Attention Mechanisms**  \n",
    "- **Bahdanau attention** (2014) improved RNNs by allowing the decoder to focus on relevant words at each step rather than relying only on a final hidden state.  \n",
    "- In 2017, researchers found that **RNNs were unnecessary** for language models and developed the **Transformer architecture**, which relies entirely on **self-attention** mechanisms.  \n",
    "\n",
    "### **3. Understanding Self-Attention**  \n",
    "- **Self-attention** enables each word in a sentence to attend to (focus on) all other words, capturing relationships between words more effectively than RNNs.  \n",
    "- **Context vectors** are generated, which enrich each word’s representation by incorporating information from other words.  \n",
    "\n",
    "### **4. Implementing a Simple Self-Attention Mechanism**  \n",
    "- The **goal** is to compute a **context vector** for each input word, weighted by its relevance to the other words.  \n",
    "- The **dot product** is used to calculate similarity scores between words.  \n",
    "- **Normalization** ensures that the attention weights sum to 1, making them interpretable.  \n",
    "- The **softmax function** is often used for stabilization.  \n",
    "\n",
    "### **5. Example Implementation in PyTorch**  \n",
    "- **Step 1: Define input tokens** (already embedded as 3D vectors).  \n",
    "- **Step 2: Compute attention scores** using the dot product between each word and a query word.  \n",
    "- **Step 3: Normalize scores to obtain attention weights**, ensuring they sum to 1.  \n",
    "- **Step 4: Compute the weighted sum to get the final context vector.**  \n",
    "\n",
    "### **Key Takeaways**  \n",
    "- **Self-attention** allows LLMs to capture relationships between words efficiently.  \n",
    "- It **overcomes the limitations** of RNNs by allowing each word to reference all others, not just previous ones.  \n",
    "- This chapter **focused on implementing self-attention**, with future sections covering trainable weights and multi-head attention.  \n",
    "\n",
    "Would you like me to explain any specific part in more detail? 😊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Scores: tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n",
      "Attention Weights after softmax: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Context Vector: tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define the input sentence as token embeddings (example 3D vectors for simplicity)\n",
    "inputs = torch.tensor([\n",
    "    [0.43, 0.15, 0.89],  # Token 1: \"Your\"\n",
    "    [0.55, 0.87, 0.66],  # Token 2: \"journey\"\n",
    "    [0.57, 0.85, 0.64],  # Token 3: \"starts\"\n",
    "    [0.22, 0.58, 0.33],  # Token 4: \"with\"\n",
    "    [0.77, 0.25, 0.10],  # Token 5: \"one\"\n",
    "    [0.05, 0.80, 0.55]   # Token 6: \"step\"\n",
    "])\n",
    "\n",
    "# Select a query token (e.g., the second token \"journey\")\n",
    "query = inputs[1]\n",
    "\n",
    "# Step 1: Compute attention scores (dot product of the query with all tokens)\n",
    "attn_scores = torch.empty(inputs.shape[0])\n",
    "for i, token in enumerate(inputs):\n",
    "    attn_scores[i] = torch.dot(query, token)\n",
    "\n",
    "# Print the attention scores\n",
    "print(\"Attention Scores:\", attn_scores)\n",
    "# it is taking dot product of \"journey\" with every other word to see how close ti is to other words ( in the loop) and \n",
    "#printing it\n",
    "\n",
    "# Step 2: Normalize the attention scores using softmax\n",
    "attn_weights = torch.nn.functional.softmax(attn_scores, dim=0)\n",
    "print(\"Attention Weights after softmax:\", attn_weights)\n",
    "#then it is sofrmaxing the entire thing to calculate the probaabilities\n",
    "\n",
    "# Step 3: Compute the context vector as a weighted sum of input embeddings\n",
    "context_vector = torch.zeros_like(query)\n",
    "for i, token in enumerate(inputs):\n",
    "    context_vector += attn_weights[i] * token\n",
    "\n",
    "print(\"Context Vector:\", context_vector)\n",
    "# The context vector we computed represents a new, enriched version of the word \"journey\" \n",
    "# (our query word), but now infused with information from the entire sentence.\n",
    "#Instead of treating \"journey\"as an isolaiid word, the context vector captures how\n",
    "#  much \"journey\" relates to other words in the sentence (e.g., \"Your\", \"starts\", \"one\", \"step\", etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5500, 0.8700, 0.6600])\n"
     ]
    }
   ],
   "source": [
    "print(inputs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, like we are calculated, the attention score of.Journey for all otherwords, we will also calculate the attention score of all other words with all other words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = torch.empty(6, 6)\n",
    "for i, x_i in enumerate(inputs):\n",
    "        for j, x_j in enumerate(inputs):\n",
    "            attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = inputs @ inputs.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 2 sum: 1.0\n",
      "All row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "row_2_sum = sum([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
    "print(\"Row 2 sum:\", row_2_sum)\n",
    "print(\"All row sums:\", attn_weights.sum(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "all_context_vecs = attn_weights @ inputs\n",
    "print(all_context_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Previous 2nd context vector:\", context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 3])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Understanding Self-Attention Step by Step (For an Absolute Beginner)**  \n",
    "\n",
    "Let's go through each of these codes carefully, assuming you're new to this. I'll explain them in a way that makes sense even if you're not familiar with deep learning.\n",
    "\n",
    "---\n",
    "\n",
    "## **CODE 1** (Basic Self-Attention Mechanism)  \n",
    "\n",
    "This code implements a simplified self-attention mechanism for one token (the word \"journey\").  \n",
    "\n",
    "### **Step 1: Defining Inputs (Words as Vectors)**  \n",
    "```python\n",
    "inputs = torch.tensor([\n",
    "    [0.43, 0.15, 0.89],  # \"Your\"\n",
    "    [0.55, 0.87, 0.66],  # \"journey\"\n",
    "    [0.57, 0.85, 0.64],  # \"starts\"\n",
    "    [0.22, 0.58, 0.33],  # \"with\"\n",
    "    [0.77, 0.25, 0.10],  # \"one\"\n",
    "    [0.05, 0.80, 0.55]   # \"step\"\n",
    "])\n",
    "```\n",
    "- Each word is represented as a **3D vector**.  \n",
    "- These vectors are just **numbers representing the meaning of words** (they are called \"embeddings\").  \n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Selecting a Query Word**\n",
    "```python\n",
    "query = inputs[1]\n",
    "```\n",
    "- We **select the word \"journey\"** as the \"query\".  \n",
    "- This means we are trying to find **how \"journey\" relates to other words** in the sentence.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3: Compute Attention Scores (Dot Product)**\n",
    "```python\n",
    "attn_scores = torch.empty(inputs.shape[0])  # Create an empty tensor to store scores\n",
    "for i, token in enumerate(inputs):\n",
    "    attn_scores[i] = torch.dot(query, token)\n",
    "```\n",
    "#### What is happening here?  \n",
    "- We are taking the **dot product** of \"journey\" with every other word in the sentence.  \n",
    "- This measures **how similar each word is to \"journey\"**.  \n",
    "- The dot product gives a **higher number if the words are similar** and a **lower number if they are different**.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Step 4: Convert Scores to Probabilities (Softmax)**\n",
    "```python\n",
    "attn_weights = torch.nn.functional.softmax(attn_scores, dim=0)\n",
    "```\n",
    "#### Why do we need this?  \n",
    "- The attention scores are just numbers. We need to **convert them into probabilities** so they sum to **1**.  \n",
    "- The **softmax function** makes sure the biggest score gets the highest probability.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Step 5: Compute the Context Vector (Weighted Sum)**\n",
    "```python\n",
    "context_vector = torch.zeros_like(query)  # Initialize the context vector\n",
    "for i, token in enumerate(inputs):\n",
    "    context_vector += attn_weights[i] * token\n",
    "```\n",
    "#### What is happening here?  \n",
    "- Each word **contributes to the final output** based on how similar it is to \"journey\".  \n",
    "- Words that are **more relevant get a bigger share** in the final output.  \n",
    "- The **context vector** is a new version of \"journey\", **but now it includes information from all words in the sentence**.  \n",
    "\n",
    "📌 **Think of this like a group discussion**:  \n",
    "- If many people talk about the same topic, that topic becomes important.  \n",
    "- The final opinion (context vector) depends on how much each person (word) contributed.  \n",
    "\n",
    "---\n",
    "\n",
    "## **CODE 2** (Computing Attention Scores for All Tokens)  \n",
    "\n",
    "```python\n",
    "attn_scores = torch.empty(6, 6)  # Create a 6x6 empty matrix\n",
    "for i, x_i in enumerate(inputs):\n",
    "        for j, x_j in enumerate(inputs):\n",
    "            attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "print(attn_scores)\n",
    "```\n",
    "### **What is happening here?**  \n",
    "- Instead of computing attention scores **only for \"journey\"**, now we compute attention scores for **every word with every other word**.  \n",
    "- The result is a **6x6 matrix**, where each row represents a word and each column represents how much attention it gives to other words.  \n",
    "\n",
    "---\n",
    "\n",
    "## **CODE 3** (Using Matrix Multiplication for Faster Computation)  \n",
    "\n",
    "```python\n",
    "attn_scores = inputs @ inputs.T\n",
    "print(attn_scores)\n",
    "```\n",
    "### **Why do this?**  \n",
    "- Instead of using **for loops**, we use **matrix multiplication** (`@`) to compute all dot products **at once**.  \n",
    "- `inputs.T` means **transpose** (swap rows and columns).  \n",
    "- The result is the **same as CODE 2, but computed much faster**.  \n",
    "\n",
    "---\n",
    "\n",
    "## **CODE 4** (Applying Softmax to Get Attention Weights)  \n",
    "\n",
    "```python\n",
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(attn_weights)\n",
    "```\n",
    "### **What is happening?**  \n",
    "- We take the **6x6 attention scores** and apply **softmax** row-wise.  \n",
    "- This converts raw scores into **probabilities** (each row sums to **1**).  \n",
    "- This tells us **how much each word attends to every other word**.  \n",
    "\n",
    "---\n",
    "\n",
    "## **CODE 5** (Verifying Softmax Normalization)  \n",
    "\n",
    "```python\n",
    "row_2_sum = sum([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
    "print(\"Row 2 sum:\", row_2_sum)\n",
    "print(\"All row sums:\", attn_weights.sum(dim=-1))\n",
    "```\n",
    "### **What is happening?**  \n",
    "- We **check if softmax is working correctly**.  \n",
    "- **Each row should sum to 1** (because softmax converts numbers into probabilities).  \n",
    "- `attn_weights.sum(dim=-1)` prints the sum of each row to confirm that.  \n",
    "\n",
    "---\n",
    "\n",
    "## **CODE 6** (Computing All Context Vectors)  \n",
    "\n",
    "```python\n",
    "all_context_vecs = attn_weights @ inputs\n",
    "print(all_context_vecs)\n",
    "```\n",
    "### **What is happening?**  \n",
    "- Before, we computed **one context vector (for \"journey\")**.  \n",
    "- Now, we compute **context vectors for all words at once** using **matrix multiplication**.  \n",
    "- This creates **a new sentence representation** where each word **is updated based on its relationship with other words**.  \n",
    "\n",
    "---\n",
    "\n",
    "## **CODE 7** (Comparing with Previous Computation)  \n",
    "\n",
    "```python\n",
    "print(\"Previous 2nd context vector:\", context_vec_2)\n",
    "```\n",
    "### **Why do this?**  \n",
    "- This checks if the context vector we computed earlier for **\"journey\"** (from CODE 1) is **the same as the second row in CODE 6**.  \n",
    "- If both are the same, it means our new method (matrix-based computation) is correct!  \n",
    "\n",
    "---\n",
    "\n",
    "## **Final Summary**  \n",
    "### **Understanding the Progression**  \n",
    "| Code | What It Does |\n",
    "|------|-------------|\n",
    "| **CODE 1** | Computes self-attention for one word (\"journey\") using loops |\n",
    "| **CODE 2** | Computes attention scores for all words using loops |\n",
    "| **CODE 3** | Optimizes CODE 2 using matrix multiplication |\n",
    "| **CODE 4** | Converts attention scores to probabilities (softmax) |\n",
    "| **CODE 5** | Verifies that softmax probabilities sum to 1 |\n",
    "| **CODE 6** | Computes context vectors for all words using matrix multiplication |\n",
    "| **CODE 7** | Checks if the optimized method matches the manual method |\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Takeaways**  \n",
    "- **Attention scores** tell us **how much each word is related to every other word**.  \n",
    "- **Softmax converts scores into probabilities** so they sum to **1**.  \n",
    "- **Context vectors combine information from all words**, making each word **aware of its surroundings**.  \n",
    "- **Using matrices speeds up computation**, making the model efficient.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Was Your Initial Understanding Correct?**  \n",
    "Your understanding was mostly right, but now you can see the **matrix-based optimization** steps. Instead of doing everything in a loop, we used **matrix multiplication** to compute **all attention scores, weights, and context vectors at once**.  \n",
    "\n",
    "Now, do you feel more confident in these codes? 😊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 2: tensor([-1.1729, -0.0048])\n"
     ]
    }
   ],
   "source": [
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2\n",
    "torch.manual_seed(123)\n",
    "w_query = torch.nn.Parameter(torch.randn(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.randn(d_in, d_out), requires_grad=False)\n",
    "w_value = torch.nn.Parameter(torch.randn(d_in, d_out), requires_grad=False)\n",
    "\n",
    "query_2 = x_2 @ w_query\n",
    "value_2 = inputs @ w_value\n",
    "key_2 = x_2 @ W_key\n",
    "\n",
    "print(\"Query 2:\", query_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(d_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“weight” is short for “weight parameters,” the val-\n",
    "ues of a neural network that are optimized during training. This is not to be confused\n",
    "with the attention weights\n",
    "\n",
    "Weight parameters are the fundamental, learned coefficients that define\n",
    "the network’s connections, while attention weights are dynamic, context-specific values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key\n",
    "values = inputs @ w_value\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can tell from the outputs, we successfully projected the six input tokens from a\n",
    "three-dimensional onto a two-dimensional embedding space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1376)\n"
     ]
    }
   ],
   "source": [
    "keys_2 = keys[1]\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(attn_score_22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.2172,  0.1376,  0.1730, -0.0491,  0.7616, -0.3809])\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2 = query_2 @ keys.T\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1704, 0.1611, 0.1652, 0.1412, 0.2505, 0.1117])\n"
     ]
    }
   ],
   "source": [
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
    "print(attn_weights_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2854, 0.4081])\n"
     ]
    }
   ],
   "source": [
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go step by step and break down **each code line by line** in a way that's super easy to understand. We'll use a **real-world analogy** wherever possible.  \n",
    "\n",
    "---\n",
    "\n",
    "# **CODE 1: Creating Query, Key, and Value Vectors**  \n",
    "```python\n",
    "x_2 = inputs[1]  # Select the 2nd word's embedding (word: \"journey\")\n",
    "\n",
    "d_in = inputs.shape[1]  # Get the number of features (3 in this case)\n",
    "d_out = 2  # We want to transform our embeddings into a smaller space (2D)\n",
    "\n",
    "torch.manual_seed(123)  # Set seed for reproducibility\n",
    "\n",
    "# Define weight matrices for Query, Key, and Value transformations\n",
    "w_query = torch.nn.Parameter(torch.randn(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.randn(d_in, d_out), requires_grad=False)\n",
    "w_value = torch.nn.Parameter(torch.randn(d_in, d_out), requires_grad=False)\n",
    "\n",
    "# Compute transformed vectors\n",
    "query_2 = x_2 @ w_query  # Transform \"journey\" into a query vector\n",
    "value_2 = inputs @ w_value  # Transform all words into value vectors\n",
    "key_2 = x_2 @ W_key  # Transform \"journey\" into a key vector\n",
    "\n",
    "print(\"Query 2:\", query_2)  # Print the query vector\n",
    "```\n",
    "\n",
    "### **What is happening here?**  \n",
    "- Each word is represented as a **3D vector (embedding)**.  \n",
    "- We want to transform them into a **smaller 2D space** using **weight matrices**.  \n",
    "- The weight matrices (`w_query`, `W_key`, `w_value`) help compute **Query, Key, and Value vectors**.  \n",
    "- Specifically:\n",
    "  - `query_2` → The transformed version of \"journey\" as a **query**.  \n",
    "  - `value_2` → The transformed versions of **all words** as **values**.  \n",
    "  - `key_2` → The transformed version of \"journey\" as a **key**.  \n",
    "\n",
    "📌 **Analogy**  \n",
    "Imagine a **teacher grading students** on different subjects:  \n",
    "- **Query** = What you are looking for (e.g., \"Who is good at math?\")  \n",
    "- **Keys** = Students' skills (Math, Science, English)  \n",
    "- **Values** = Students' detailed reports (full information)  \n",
    "\n",
    "This step **converts raw student data into a form we can use for searching**.  \n",
    "\n",
    "---\n",
    "\n",
    "# **CODE 2: Compute Keys and Values for All Words**  \n",
    "```python\n",
    "keys = inputs @ W_key  # Transform all words into keys\n",
    "values = inputs @ w_value  # Transform all words into values\n",
    "```\n",
    "\n",
    "### **What is happening here?**  \n",
    "- Instead of transforming **only \"journey\"**, we now transform **all words** into:\n",
    "  - **Keys** (for comparison)\n",
    "  - **Values** (for final representation)  \n",
    "\n",
    "📌 **Analogy**  \n",
    "Now, instead of looking at only one student (\"journey\"), we **assess all students** (all words) and store their:  \n",
    "- **Skills** (Keys)  \n",
    "- **Detailed performance reports** (Values)  \n",
    "\n",
    "Now, each word has its **own key and value representation**.  \n",
    "\n",
    "---\n",
    "\n",
    "# **CODE 3: Compute Similarity Between \"Journey\" and Its Own Key**  \n",
    "```python\n",
    "keys_2 = keys[1]  # Extract the key vector for \"journey\"\n",
    "attn_score_22 = query_2.dot(keys_2)  # Compute dot product (similarity)\n",
    "print(attn_score_22)\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)\n",
    "```\n",
    "\n",
    "### **What is happening here?**  \n",
    "- We take the **key** corresponding to \"journey\" (`keys_2`).  \n",
    "- We **compare \"journey's\" query with its own key** using the **dot product**.  \n",
    "- This gives us **a similarity score** (higher = more similar).  \n",
    "\n",
    "📌 **Analogy**  \n",
    "If a student is **good at math**, we check **how much their skill profile matches their math abilities**.  \n",
    "This tells us **how well they match their own description**.  \n",
    "\n",
    "---\n",
    "\n",
    "# **CODE 4: Compute Attention Scores for All Words**  \n",
    "```python\n",
    "attn_scores_2 = query_2 @ keys.T  # Compute attention scores for all words\n",
    "print(attn_scores_2)\n",
    "```\n",
    "\n",
    "### **What is happening here?**  \n",
    "- Instead of **just comparing \"journey\" with itself**, we now compare \"journey\" **with every word in the sentence**.  \n",
    "- This gives us **a list of similarity scores** for \"journey\" **vs every other word**.  \n",
    "\n",
    "📌 **Analogy**  \n",
    "Instead of checking **only one student's skill match**, we now compare this student **with all other students** to see who is similar to them.  \n",
    "\n",
    "📌 **Output**  \n",
    "A **list of numbers**, each representing **how much \"journey\" relates to another word**.  \n",
    "\n",
    "---\n",
    "\n",
    "# **CODE 5: Apply Softmax for Normalized Attention Weights**  \n",
    "```python\n",
    "d_k = keys.shape[-1]  # Get the number of features in keys\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)  # Normalize scores\n",
    "print(attn_weights_2)\n",
    "```\n",
    "\n",
    "### **What is happening here?**  \n",
    "- The **raw attention scores** are just numbers.  \n",
    "- We **normalize them** using **softmax** to get probabilities.  \n",
    "- The division `d_k**0.5` helps stabilize values.  \n",
    "\n",
    "📌 **Analogy**  \n",
    "If we give students **raw scores**, they might not make sense.  \n",
    "- We **convert them into percentages** (so they sum to 100%).  \n",
    "- Now, we **know how much \"journey\" should focus on each word**.  \n",
    "\n",
    "📌 **Output**  \n",
    "A **list of probabilities**, showing **how much attention \"journey\" gives to each word**.  \n",
    "\n",
    "---\n",
    "\n",
    "# **CODE 6: Compute the Final Context Vector for \"Journey\"**  \n",
    "```python\n",
    "context_vec_2 = attn_weights_2 @ values  # Multiply attention weights with values\n",
    "print(context_vec_2)\n",
    "```\n",
    "\n",
    "### **What is happening here?**  \n",
    "- We **multiply the attention weights with the value vectors**.  \n",
    "- This creates a **new representation of \"journey\" that includes information from all words**.  \n",
    "\n",
    "📌 **Analogy**  \n",
    "Imagine writing an **essay**.  \n",
    "- Initially, you write each sentence **independently**.  \n",
    "- After reviewing the whole essay, you **refine each sentence** by considering the full context.  \n",
    "- Now, **each sentence makes more sense in the overall story**.  \n",
    "\n",
    "📌 **Output**  \n",
    "A **new vector for \"journey\"** that has **learned from all other words** in the sentence.  \n",
    "\n",
    "---\n",
    "\n",
    "# **Final Summary**\n",
    "| Code | What It Does | Simple Explanation |\n",
    "|------|-------------|--------------------|\n",
    "| **Code 1** | Create query, key, and value vectors | Convert words into \"searchable\" forms |\n",
    "| **Code 2** | Compute keys and values for all words | Assign skills (keys) and reports (values) to all words |\n",
    "| **Code 3** | Compute similarity of \"journey\" with itself | Check if a student matches their own skills |\n",
    "| **Code 4** | Compute similarity with all words | Compare \"journey\" with every word in the sentence |\n",
    "| **Code 5** | Convert scores into probabilities (softmax) | Normalize scores so they sum to 1 |\n",
    "| **Code 6** | Compute final enriched word representation | Update \"journey\" with information from all words |\n",
    "\n",
    "---\n",
    "\n",
    "# **Final Takeaways**\n",
    "- **Query** = What we are searching for.  \n",
    "- **Key** = Information about each word.  \n",
    "- **Value** = The actual representation of each word.  \n",
    "- **Attention scores** show how words relate to each other.  \n",
    "- **Softmax converts scores into probabilities**.  \n",
    "- **Context vector updates words with information from the entire sentence**.  \n",
    "\n",
    "💡 **Now you understand self-attention!** This is how models like **ChatGPT, BERT, and Transformers** learn meaning from text. 🚀  \n",
    "\n",
    "Did this explanation help? Let me know if anything needs further clarification! 😊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in,d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in,d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in,d_out))\n",
    "    def forward(self, x ):\n",
    "            keys = x @ self.W_key\n",
    "            values = x @ self.W_value\n",
    "            queries = x @ self.W_query\n",
    "            attn_scores = queries @ keys.T\n",
    "            attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "            context_vec= attn_weights @ values\n",
    "            return context_vec\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The thing is that this attention module is still missing twk kwy components \n",
    " 1) The masking mechanism is misssing \n",
    " 2) The multiheaded module is not implemented \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Masking Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "context_length = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "print(mask_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can multiply this mask with the attention weights to zero-out the values above\n",
    "the diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1385, 0.2379, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1390, 0.2369, 0.2326, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.0000, 0.0000],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.0000],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "masked_simple = attn_weights*mask_simple\n",
    "print(masked_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the elements above the diagonal are successfully zeroed out:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3680, 0.6320, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2284, 0.3893, 0.3822, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2046, 0.2956, 0.2915, 0.2084, 0.0000, 0.0000],\n",
      "        [0.1753, 0.2250, 0.2269, 0.1570, 0.2158, 0.0000],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "row_sums = masked_simple.sum(dim = -1 , keepdim= True)\n",
    "masked_simple_norm = masked_simple/ row_sums\n",
    "print(masked_simple_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third step is to renormalize the attention weights to sum up to 1 again in each\n",
    "row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.9544, 1.4950,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.9422, 1.4754, 1.4570,   -inf,   -inf,   -inf],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937,   -inf,   -inf],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654,   -inf],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4056, 0.5944, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2566, 0.3741, 0.3693, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2176, 0.2823, 0.2796, 0.2205, 0.0000, 0.0000],\n",
      "        [0.1826, 0.2178, 0.2191, 0.1689, 0.2115, 0.0000],\n",
      "        [0.1473, 0.2033, 0.1996, 0.1500, 0.1160, 0.1839]])\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📌 **How is the Causal Attention Mask Applied? (Step-by-Step Explanation)**  \n",
    "\n",
    "Causal attention ensures that a token (word) in a sequence **can only attend to itself and the tokens before it** but **not future tokens** (to prevent information leakage). This is **especially important in autoregressive models** like GPT.  \n",
    "\n",
    "Let's break down the **entire process step by step**:\n",
    "\n",
    "---\n",
    "\n",
    "## **1️⃣ Compute Attention Scores and Weights**  \n",
    "\n",
    "We first compute the **attention scores** and apply the **softmax function** to obtain attention weights:\n",
    "\n",
    "```python\n",
    "queries = sa_v2.W_query(inputs)  # Compute query vectors\n",
    "keys = sa_v2.W_key(inputs)  # Compute key vectors\n",
    "attn_scores = queries @ keys.T  # Compute attention scores (dot product of queries and keys)\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)  # Normalize with softmax\n",
    "```\n",
    "\n",
    "### 🔹 **What’s Happening Here?**\n",
    "- `attn_scores` is a **matrix** that tells us **how much each word attends to every other word**.  \n",
    "- `torch.softmax(..., dim=-1)` ensures that **each row sums to 1**, meaning that each word distributes its attention across other words.\n",
    "\n",
    "### 🔹 **Example Output (`attn_weights`)**\n",
    "```\n",
    "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
    "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
    "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
    "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
    "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
    "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]])\n",
    "```\n",
    "🔹 **Problem:** Words are attending to **future words**, which **we don’t want** in causal attention.  \n",
    "\n",
    "---\n",
    "\n",
    "## **2️⃣ Create a Lower Triangular Mask**\n",
    "We now create a **mask** to ensure each token **only attends to itself and past tokens**.\n",
    "\n",
    "```python\n",
    "context_length = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "print(mask_simple)\n",
    "```\n",
    "\n",
    "### 🔹 **What’s Happening Here?**\n",
    "- `torch.tril()` creates a **lower triangular matrix** where:  \n",
    "  - **1s** are for allowed positions (tokens can attend to these).  \n",
    "  - **0s** are for disallowed positions (tokens **cannot** attend to future tokens).\n",
    "\n",
    "### 🔹 **Example Output (`mask_simple`)**\n",
    "```\n",
    "tensor([[1., 0., 0., 0., 0., 0.],\n",
    "        [1., 1., 0., 0., 0., 0.],\n",
    "        [1., 1., 1., 0., 0., 0.],\n",
    "        [1., 1., 1., 1., 0., 0.],\n",
    "        [1., 1., 1., 1., 1., 0.],\n",
    "        [1., 1., 1., 1., 1., 1.]])\n",
    "```\n",
    "🔹 **Effect:**  \n",
    "- The **first word** can only attend to itself.  \n",
    "- The **second word** can attend to itself and the first word.  \n",
    "- The **third word** can attend to the first two, and so on.  \n",
    "- **Future words are masked (0s).**  \n",
    "\n",
    "---\n",
    "\n",
    "## **3️⃣ Apply the Mask to Attention Weights**\n",
    "Now we **apply the mask** by multiplying it element-wise (`*` operator):\n",
    "\n",
    "```python\n",
    "masked_simple = attn_weights * mask_simple\n",
    "print(masked_simple)\n",
    "```\n",
    "\n",
    "### 🔹 **Effect:**  \n",
    "- Any value **above the diagonal becomes 0** (future words are ignored).  \n",
    "\n",
    "### 🔹 **Example Output (`masked_simple`)**\n",
    "```\n",
    "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
    "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
    "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
    "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]])\n",
    "```\n",
    "🔹 **Problem:** After masking, **the row sums are no longer 1**.  \n",
    "\n",
    "---\n",
    "\n",
    "## **4️⃣ Renormalize Attention Weights**\n",
    "Since masking removed some values, we **renormalize** to ensure each row still sums to 1:\n",
    "\n",
    "```python\n",
    "row_sums = masked_simple.sum(dim=-1, keepdim=True)\n",
    "masked_simple_norm = masked_simple / row_sums\n",
    "print(masked_simple_norm)\n",
    "```\n",
    "\n",
    "### 🔹 **Effect:**  \n",
    "- Each row’s sum **is forced back to 1**, making it a **valid probability distribution**.  \n",
    "\n",
    "### 🔹 **Example Output (`masked_simple_norm`)**\n",
    "```\n",
    "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
    "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
    "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
    "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]])\n",
    "```\n",
    "🔹 **Now the masked attention weights sum to 1** in each row! ✅  \n",
    "\n",
    "---\n",
    "\n",
    "## **5️⃣ A More Efficient Trick: Using -∞ Instead of Multiplication**\n",
    "Instead of using `0s` in the mask and manually renormalizing, we can **directly use -∞ before applying softmax**:\n",
    "\n",
    "```python\n",
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)  # Upper triangular mask\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)  # Replace upper triangle with -∞\n",
    "print(masked)\n",
    "```\n",
    "\n",
    "🔹 **Effect:**  \n",
    "- `masked_fill()` replaces **future positions with -∞**.  \n",
    "- When we apply `softmax()`, those positions **become 0 automatically**.\n",
    "\n",
    "### 🔹 **Example Output (`masked`)**\n",
    "```\n",
    "tensor([[0.2899, -inf, -inf, -inf, -inf, -inf],\n",
    "        [0.4656, 0.1723, -inf, -inf, -inf, -inf],\n",
    "        [0.4594, 0.1703, 0.1731, -inf, -inf, -inf],\n",
    "        [0.2642, 0.1024, 0.1036, 0.0186, -inf, -inf],\n",
    "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786, -inf],\n",
    "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]])\n",
    "```\n",
    "\n",
    "Now applying `softmax()`:\n",
    "\n",
    "```python\n",
    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1)\n",
    "```\n",
    "\n",
    "🔹 **This is faster & avoids manual renormalization!** 🚀  \n",
    "\n",
    "---\n",
    "\n",
    "### ✅ **Final Thoughts**\n",
    "- **Causal masking prevents future words from influencing past words.**\n",
    "- **Multiplying with 0s works, but using -∞ is more efficient.**\n",
    "- **Renormalization ensures valid attention probabilities.**\n",
    "- **Softmax automatically handles the masked values.** 🎯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking additional attention weights with dropout\n",
    "\n",
    "Dropout in deep learning is a technique where randomly selected hidden layer units\n",
    "are ignored during training, effectively “dropping” them out. This method helps pre-\n",
    "vent overfitting by ensuring that a model does not become overly reliant on any spe-\n",
    "cific set of hidden layer units. It’s important to emphasize that dropout is only used\n",
    "during training and is disabled afterward.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2., 2., 2., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.],\n",
      "        [0., 0., 2., 0., 2., 0.],\n",
      "        [2., 2., 0., 0., 0., 2.],\n",
      "        [2., 0., 0., 0., 0., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5)\n",
    "example = torch.ones(6,6)\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When applying dropout to an attention weight matrix with a rate of 50%, half of the\n",
    "elements in the matrix are randomly set to zero. To compensate for the reduction in\n",
    "active elements, the values of the remaining elements in the matrix are scaled up by a\n",
    "factor of 1/0.5 = 2. This scaling is crucial to maintain the overall balance of the attention weights, ensuring that the average influence of the attention mechanism remains\n",
    "consistent during both the training and inference phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.1888, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.7386, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4352, 0.5646, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3652, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4065, 0.0000, 0.0000, 0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But before we begin, let’s ensure that the code can handle batches consisting of\n",
    "more than one input so that the CausalAttention class supports the batch outputs\n",
    "produced by the data loader we implemented in chapter 2.\n",
    "For simplicity, to simulate such batch inputs, we duplicate the input text example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):  # ✅ Correct spelling\n",
    "    def __init__(self, d_in , d_out, context_length, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        attn_scores = queries @ keys.transpose(1, 2)\n",
    "        attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3])\n"
     ]
    }
   ],
   "source": [
    "print(inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vecs= ca(batch)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4300, 0.1500, 0.8900],\n",
      "         [0.5500, 0.8700, 0.6600],\n",
      "         [0.5700, 0.8500, 0.6400],\n",
      "         [0.2200, 0.5800, 0.3300],\n",
      "         [0.7700, 0.2500, 0.1000],\n",
      "         [0.0500, 0.8000, 0.5500]],\n",
      "\n",
      "        [[0.4300, 0.1500, 0.8900],\n",
      "         [0.5500, 0.8700, 0.6600],\n",
      "         [0.5700, 0.8500, 0.6400],\n",
      "         [0.2200, 0.5800, 0.3300],\n",
      "         [0.7700, 0.2500, 0.1000],\n",
      "         [0.0500, 0.8000, 0.5500]]])\n"
     ]
    }
   ],
   "source": [
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(context_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing Muti-Headed Atention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in,d_out, context_length,dropout,num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads=nn.ModuleList([CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) for _ in range(num_heads)])\n",
    "    def forward(self,x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1] # This is the number of tokens\n",
    "d_in, d_out = 3,2\n",
    "mha = MultiHeadAttentionWrapper(\n",
    "d_in, d_out, context_length, 0.0, num_heads=2\n",
    ")\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulatHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out,context_length , dropout, num_heads, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask' , torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "    def forward(self , x):\n",
    "        b, num_tokens,d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        keys = keys.view(b, num_tokens,self.num_heads,self.head_dim)\n",
    "        values = values.view(b, num_tokens,self.num_heads,self.head_dim)\n",
    "        queries = queries.view(b, num_tokens,self.num_heads,self.head_dim)\n",
    "        keys = keys.transpose(1,2)\n",
    "        queries = queries.transpose(1,2)\n",
    "        values = values.transpose(1,2)\n",
    "        attn_scores = queries @ keys.transpose(-2,-1)\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vec= (attn_weights@values).transpose(1,2)\n",
    "        contex_vec  = context_vec.contigious().view(b, num_tokens,self.d_out)\n",
    "        contex_vec= contex_vec.out_proj(contex_vec)\n",
    "        return contex_vec\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Understanding Reshaping (`view()`) in Multi-Head Attention**  \n",
    "Let's break these lines down **step by step** and explain why we reshape the tensors.\n",
    "\n",
    "---\n",
    "\n",
    "## **1️⃣ Reshape `keys`, `values`, and `queries`**\n",
    "```python\n",
    "keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "```\n",
    "\n",
    "### ✅ **What is happening?**\n",
    "- `keys`, `values`, and `queries` are originally **(batch_size, num_tokens, d_out)**.\n",
    "- `view()` **reshapes them** into **(batch_size, num_tokens, num_heads, head_dim)**.\n",
    "\n",
    "### 🛠 **Breaking It Down**\n",
    "| Variable | Original Shape | After `view()` | Why? |\n",
    "|----------|---------------|----------------|------|\n",
    "| `keys` / `values` / `queries` | `(batch_size, num_tokens, d_out)` | `(batch_size, num_tokens, num_heads, head_dim)` | Splits `d_out` into `num_heads × head_dim` |\n",
    "\n",
    "### ❓ **Why do we do this?**\n",
    "- In **multi-head attention**, instead of using a single attention mechanism, we **split** `d_out` into multiple **heads**.\n",
    "- Each head learns **different attention patterns** by focusing on different aspects of the input.\n",
    "\n",
    "---\n",
    "\n",
    "## **2️⃣ Why `d_out = num_heads * head_dim`?**\n",
    "When defining the multi-head attention layer, we ensure:\n",
    "```python\n",
    "assert (d_out % num_heads == 0), \"d_out must be divisible by num_heads\"\n",
    "```\n",
    "This ensures that:\n",
    "```\n",
    "head_dim = d_out / num_heads\n",
    "```\n",
    "Each attention head gets a **smaller slice of the total embedding dimension**.\n",
    "\n",
    "---\n",
    "\n",
    "## **🛠 Example**\n",
    "Suppose:\n",
    "- `batch_size = 2`\n",
    "- `num_tokens = 4`\n",
    "- `d_out = 8`\n",
    "- `num_heads = 2`\n",
    "- Then, `head_dim = d_out / num_heads = 8 / 2 = 4`\n",
    "\n",
    "Before reshaping (`keys.shape`):  \n",
    "```\n",
    "(batch_size, num_tokens, d_out) = (2, 4, 8)\n",
    "```\n",
    "After reshaping (`keys.shape`):  \n",
    "```\n",
    "(batch_size, num_tokens, num_heads, head_dim) = (2, 4, 2, 4)\n",
    "```\n",
    "Now, each of the **2 heads** gets a 4-dimensional representation per token.\n",
    "\n",
    "---\n",
    "\n",
    "## **🚀 Summary of Each Line**\n",
    "| Line | What Happens? | Why? |\n",
    "|------|-------------|------|\n",
    "| `keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)` | Reshapes keys into multiple heads | Allows **each head** to learn different attention patterns |\n",
    "| `values = values.view(b, num_tokens, self.num_heads, self.head_dim)` | Reshapes values for multi-head attention | Ensures each head gets its own representation |\n",
    "| `queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)` | Reshapes queries for multi-head attention | Allows each head to compute attention separately |\n",
    "\n",
    "This step **prepares the data** for **multi-head attention**, ensuring each head gets its own independent representation.\n",
    "\n",
    "Let me know if anything needs more explanation! 🚀\n",
    "1. **The total number of elements must stay the same** before and after reshaping.  \n",
    "   - Example: If you have a tensor of shape `(6,)` with 6 elements, you can reshape it into `(2, 3)`, but you can't reshape it to `(2, 4)` because that would require 8 elements.\n",
    "\n",
    "2. `view()` does not change the **values inside the tensor**, it only **changes the structure** (how the values are grouped).\n",
    "\n",
    "### **Why is it used in your code?**\n",
    "In your multi-head attention code, we use `view()` to **reshape** the `keys`, `queries`, and `values` so that each can be split into multiple **attention heads**:\n",
    "```python\n",
    "keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "```\n",
    "- **Before:** The `keys` tensor is shaped like `(batch_size, num_tokens, d_out)`.\n",
    "- **After:** It is reshaped to `(batch_size, num_tokens, num_heads, head_dim)`.\n",
    "\n",
    "This allows each head to independently handle a smaller part of the data. It's like **splitting the work among multiple workers** (attention heads), so each one can focus on a different aspect.\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "### **1️⃣ Transposing Keys, Queries, and Values**\n",
    "```python\n",
    "keys = keys.transpose(1,2)\n",
    "queries = queries.transpose(1,2)\n",
    "values = values.transpose(1,2)\n",
    "```\n",
    "#### ✅ **What Happens Here?**\n",
    "- The shape of `keys`, `queries`, and `values` before transposing is:\n",
    "  ```\n",
    "  (batch_size, num_tokens, num_heads, head_dim)\n",
    "  ```\n",
    "- After `.transpose(1,2)`, the shape becomes:\n",
    "  ```\n",
    "  (batch_size, num_heads, num_tokens, head_dim)\n",
    "  ```\n",
    "\n",
    "#### ❓ **Why Do We Do This?**\n",
    "- We need to compute **attention scores** separately for each head.\n",
    "- Moving `num_heads` to the second dimension allows parallel computation over multiple heads.\n",
    "\n",
    "---\n",
    "\n",
    "### **2️⃣ Compute Attention Scores**\n",
    "```python\n",
    "attn_scores = queries @ keys.transpose(-2,-1)\n",
    "```\n",
    "#### ✅ **What Happens Here?**\n",
    "- `keys.transpose(-2,-1)` swaps the last two dimensions, changing its shape:\n",
    "  ```\n",
    "  (batch_size, num_heads, head_dim, num_tokens)\n",
    "  ```\n",
    "- Now, when we do:\n",
    "  ```\n",
    "  attn_scores = queries @ keys.transpose(-2,-1)\n",
    "  ```\n",
    "  - This performs **matrix multiplication** between:\n",
    "    ```\n",
    "    (batch_size, num_heads, num_tokens, head_dim)  \n",
    "    @ (batch_size, num_heads, head_dim, num_tokens)\n",
    "    ```\n",
    "  - The result is:\n",
    "    ```\n",
    "    (batch_size, num_heads, num_tokens, num_tokens)\n",
    "    ```\n",
    "  - This matrix contains **attention scores** indicating how much each token should attend to every other token.\n",
    "\n",
    "#### ❓ **Why Do We Do This?**\n",
    "- We measure how similar each query (`Q`) is to each key (`K`).\n",
    "- This similarity score determines how much focus each token should have on others.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "```python\n",
    "keys = keys.transpose(1, 2)\n",
    "queries = queries.transpose(1, 2)\n",
    "values = values.transpose(1, 2)\n",
    "\n",
    "attn_scores = queries @ keys.transpose(-2, -1)\n",
    "mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "context_vec = self.out_proj(context_vec)  # ✅ Correct function call\n",
    "\n",
    "return context_vec\n",
    "```\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "### **🔹 Compute Attention Scores**\n",
    "```python\n",
    "attn_scores = queries @ keys.transpose(-2, -1)\n",
    "```\n",
    "- Computes attention scores:  \n",
    "  - `queries @ keys^T` → `(b, num_heads, num_tokens, num_tokens)`.  \n",
    "  - Measures how well each token attends to every other token.  \n",
    "\n",
    "---\n",
    "\n",
    "### **🔹 Apply Causal Mask**\n",
    "```python\n",
    "mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "```\n",
    "- **Mask future tokens** by setting unwanted values to `-inf` (ensures no information flows from future tokens).  \n",
    "\n",
    "**🚨 Bug:**\n",
    "```python\n",
    "attn_scores.masked_fill_masked_fill_(mask_bool, -torch.inf)  # ❌ Typo\n",
    "```\n",
    "- `masked_fill_masked_fill_` is **incorrect**. The correct line is:\n",
    "  ```python\n",
    "  attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "  ```  \n",
    "\n",
    "---\n",
    "\n",
    "### **🔹 Compute Attention Weights**\n",
    "```python\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "```\n",
    "- Uses **softmax** to normalize scores into probabilities.  \n",
    "- Dividing by `sqrt(head_dim)` stabilizes gradients.  \n",
    "\n",
    "---\n",
    "\n",
    "### **🔹 Apply Dropout to Attention Weights**\n",
    "```python\n",
    "attn_weights = self.dropout(attn_weights)\n",
    "```\n",
    "- Randomly drops some attention weights (helps generalization).  \n",
    "\n",
    "---\n",
    "Sure! Let's go **line by line**, explaining what is happening and why we do it in a simple way.  \n",
    "\n",
    "---\n",
    "\n",
    "### **1️⃣ Compute Context Vector from Attention Weights**\n",
    "```python\n",
    "context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "```\n",
    "#### ✅ **What is happening?**\n",
    "- `attn_weights` (the attention probabilities) is multiplied with `values` (the input representations).\n",
    "- This gives a **weighted sum of values** for each token, based on how much attention it should give to other tokens.\n",
    "\n",
    "#### ❓ **Why do we do this?**\n",
    "- This step **creates new representations** for each token by combining information from other tokens.\n",
    "- The model decides which tokens are important and **blends their information together**.\n",
    "\n",
    "#### 🛠 **Transpose Explanation**\n",
    "```python\n",
    ".transpose(1, 2)\n",
    "```\n",
    "- This swaps the second and third dimensions.\n",
    "- Changes shape from:\n",
    "  ```\n",
    "  (batch_size, num_heads, num_tokens, head_dim)\n",
    "  ```\n",
    "  - to  \n",
    "  ```\n",
    "  (batch_size, num_tokens, num_heads, head_dim)\n",
    "  ```\n",
    "- **Why?** → Makes it easier to reshape in the next step.\n",
    "\n",
    "---\n",
    "\n",
    "### **2️⃣ Reshape to Merge Attention Heads**\n",
    "```python\n",
    "context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "```\n",
    "#### ✅ **What is happening?**\n",
    "- `.contiguous()` ensures memory is arranged properly before reshaping.\n",
    "- `.view(b, num_tokens, self.d_out)` reshapes the tensor from:\n",
    "  ```\n",
    "  (batch_size, num_tokens, num_heads, head_dim)\n",
    "  ```\n",
    "  - to  \n",
    "  ```\n",
    "  (batch_size, num_tokens, d_out)\n",
    "  ```\n",
    "  where `d_out = num_heads * head_dim`.\n",
    "\n",
    "#### ❓ **Why do we do this?**\n",
    "- Each attention **head** computed a separate representation.\n",
    "- Now, we **combine all heads into a single representation** per token.\n",
    "\n",
    "---\n",
    "\n",
    "### **3️⃣ Apply Final Linear Transformation**\n",
    "```python\n",
    "context_vec = self.out_proj(context_vec)\n",
    "```\n",
    "#### ✅ **What is happening?**\n",
    "- `self.out_proj` is a **fully connected linear layer** (`nn.Linear`) that applies a transformation to `context_vec`.\n",
    "\n",
    "#### ❓ **Why do we do this?**\n",
    "- The **attention output still needs to match the expected input size** for the next layer in the transformer.\n",
    "- This **refines the information** before passing it to the next layers.\n",
    "\n",
    "---\n",
    "\n",
    "### **🚀 Summary of Each Line**\n",
    "| Line | What Happens? | Why? |\n",
    "|------|-------------|------|\n",
    "| `context_vec = (attn_weights @ values).transpose(1, 2)` | Computes weighted sum of values based on attention | Lets each token \"learn\" from others |\n",
    "| `context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)` | Merges multiple attention heads into one representation | Combines information from all heads |\n",
    "| `context_vec = self.out_proj(context_vec)` | Applies a final transformation | Ensures the output is in the right format for the next layer |\n",
    "\n",
    "Let me know if anything needs more explanation! 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
