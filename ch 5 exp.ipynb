{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10612478,"sourceType":"datasetVersion","datasetId":6570095}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"GPT_CONFIG_124M = {\n\"vocab_size\": 50257, # Vocabulary size\n\"context_length\": 256, # Context length\n\"emb_dim\": 768, # Embedding dimension\n\"n_heads\": 12, # Number of attention heads\n\"n_layers\": 8, # Number of layers\n\"drop_rate\": 0.6, # Dropout rate\n\"qkv_bias\": False # Query-Key-Value bias\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T09:52:25.339370Z","iopub.execute_input":"2025-01-30T09:52:25.339674Z","iopub.status.idle":"2025-01-30T09:52:25.343916Z","shell.execute_reply.started":"2025-01-30T09:52:25.339653Z","shell.execute_reply":"2025-01-30T09:52:25.342892Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# ch 1 & 2\nimport urllib.request\nimport torch\nimport tiktoken  \nfrom torch.utils.data import Dataset, DataLoader\n\n# Download the text file\n#url = (\"https://raw.githubusercontent.com/rasbt/\" \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\" \"the-verdict.txt\")\n#file_path = \"the-verdict.txt\"\n#urllib.request.urlretrieve(url, file_path)\n\n# Load and tokenize the text\nwith open(r\"/kaggle/input/acrsvda/train.txt\", \"r\", encoding=\"utf-8\") as f:\n    raw_text = f.read()\n\ntokenizer = tiktoken.get_encoding(\"cl100k_base\")  \nenc_text = tokenizer.encode(raw_text)\n\n# Define a custom dataset class\nclass GPTDatasetV1(Dataset):\n    def __init__(self, txt, tokenizer, max_length, stride):\n        self.input_ids = []\n        self.target_ids = []\n        token_ids = tokenizer.encode(txt)\n        for i in range(0, len(token_ids) - max_length, stride):\n            input_chunk = token_ids[i:i + max_length]\n            target_chunk = token_ids[i + 1: i + max_length + 1]\n            self.input_ids.append(torch.tensor(input_chunk))\n            self.target_ids.append(torch.tensor(target_chunk))\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        return self.input_ids[idx], self.target_ids[idx]\n\n# Function to create a DataLoader\ndef create_dataloader_v1(txt, batch_size, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n    tokenizer = tiktoken.get_encoding(\"gpt2\")\n    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n    return dataloader \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T09:52:25.345100Z","iopub.execute_input":"2025-01-30T09:52:25.345347Z","iopub.status.idle":"2025-01-30T09:52:25.518152Z","shell.execute_reply.started":"2025-01-30T09:52:25.345325Z","shell.execute_reply":"2025-01-30T09:52:25.517307Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"\n# Create DataLoader\ndataloader = create_dataloader_v1(raw_text, batch_size=1, max_length=4, stride=2, shuffle=False)\ndata_iter = iter(dataloader)\nfirst_batch = next(data_iter)\nsecond_batch = next(data_iter)\nprint(\"First batch:\", first_batch)\nprint(\"Second batch:\", second_batch)\n\n# --- Adding Token and Positional Embeddings ---\n\n# Define token embedding layer\nvocab_size = 50257  # Typical size for GPT models\nembedding_dim = 256  # Example embedding size (GPT-3 uses 12,288)\ntoken_embedding_layer = torch.nn.Embedding(vocab_size, embedding_dim)\n\n# Convert token IDs into token embeddings\ntoken_embeddings = token_embedding_layer(first_batch[0])  # First batch of inputs\nprint(\"Token Embeddings Shape:\", token_embeddings.shape)\n\n# Define positional embedding layer\nmax_length = 4  # Same as the max sequence length\npos_embedding_layer = torch.nn.Embedding(max_length, embedding_dim)\n\n# Generate position embeddings\npositions = torch.arange(max_length).unsqueeze(0)  # Create position indices\npos_embeddings = pos_embedding_layer(positions)\nprint(\"Positional Embeddings Shape:\", pos_embeddings.shape)\n\n# Combine token and positional embeddings\ninput_embeddings = token_embeddings + pos_embeddings\nprint(\"Final Input Embeddings Shape:\", input_embeddings.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T09:52:25.519278Z","iopub.execute_input":"2025-01-30T09:52:25.519563Z","iopub.status.idle":"2025-01-30T09:52:28.291957Z","shell.execute_reply.started":"2025-01-30T09:52:25.519534Z","shell.execute_reply":"2025-01-30T09:52:28.291067Z"}},"outputs":[{"name":"stdout","text":"First batch: [tensor([[3198, 1110,   11,  257]]), tensor([[1110,   11,  257, 1310]])]\nSecond batch: [tensor([[  11,  257, 1310, 2576]]), tensor([[ 257, 1310, 2576, 3706]])]\nToken Embeddings Shape: torch.Size([1, 4, 256])\nPositional Embeddings Shape: torch.Size([1, 4, 256])\nFinal Input Embeddings Shape: torch.Size([1, 4, 256])\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"# ch 3\nimport torch.nn as nn # type: ignore\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_in, d_out,context_length , dropout, num_heads, qkv_bias = False):\n        super().__init__()\n        assert (d_out % num_heads == 0), \\\n            \"d_out must be divisible by num_heads\"\n        self.d_out = d_out\n        self.num_heads = num_heads\n        self.head_dim = d_out // num_heads \n        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.out_proj = nn.Linear(d_out, d_out)\n        self.dropout = nn.Dropout(dropout)\n        self.register_buffer('mask' , torch.triu(torch.ones(context_length, context_length), diagonal=1))\n    def forward(self , x):\n        b, num_tokens,d_in = x.shape\n        keys = self.W_key(x)\n        queries = self.W_query(x)\n        values = self.W_value(x)\n        keys = keys.view(b, num_tokens,self.num_heads,self.head_dim)\n        values = values.view(b, num_tokens,self.num_heads,self.head_dim)\n        queries = queries.view(b, num_tokens,self.num_heads,self.head_dim)\n        keys = keys.transpose(1,2)\n        queries = queries.transpose(1,2)\n        values = values.transpose(1,2)\n        attn_scores = queries @ keys.transpose(-2,-1)\n        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n        attn_scores.masked_fill_(mask_bool, -torch.inf)\n        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n        attn_weights = self.dropout(attn_weights)\n        context_vec = (attn_weights @ values).transpose(1, 2)\n        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n        context_vec = self.out_proj(context_vec)  # Correct method call\n        return context_vec\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T09:52:28.293540Z","iopub.execute_input":"2025-01-30T09:52:28.293777Z","iopub.status.idle":"2025-01-30T09:52:28.303316Z","shell.execute_reply.started":"2025-01-30T09:52:28.293755Z","shell.execute_reply":"2025-01-30T09:52:28.302308Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# ch 4\n\nclass LayerNorm(nn.Module):\n    def __init__(self, emb_dim):\n        super().__init__()\n        self.eps = 1e-5\n        self.scale = nn.Parameter(torch.ones(emb_dim))\n        self.shift = nn.Parameter(torch.zeros(emb_dim))\n    def forward(self,x):\n        mean = x.mean(dim = -1, keepdim = True)\n        var = x.var(dim = -1, keepdim = True)\n        norm_x=(x-mean)/ torch.sqrt(var+self.eps)\n        return norm_x*self.scale+self.shift\nclass GELU(nn.Module):\n    def forward(self,x):\n        return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))))\n\nclass FeedForward(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n            GELU(),\n            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n        )\n    def forward(self,x):\n        return self.layers(x)\nclass ShortcutConnection(nn.Module):\n    def __init__(self,layer_size,use_shortcut):\n        super().__init__()\n        self.use_shortcut = use_shortcut\n        self.layers = nn.ModuleList([\n            nn.Sequential(nn.Linear(layer_size[0] , layer_size[1]),\n                         GELU() ),\n             nn.Sequential(nn.Linear(layer_size[0] , layer_size[1]),\n                         GELU() ),\n             nn.Sequential(nn.Linear(layer_size[0] , layer_size[1]),\n                         GELU() ),\n             nn.Sequential(nn.Linear(layer_size[0] , layer_size[1]),\n                         GELU() ),\n        ])\n    def forward(self,x):\n            for layer in self.layers:\n                layer_output = layer(x)\n                if self.use_shortcut and x.shape == layer_output.shape:\n                    x = x + layer_output\n                else:\n                    x = layer_output\n            return x\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.att = MultiHeadAttention(\n             d_in=cfg[\"emb_dim\"],\n             d_out=cfg[\"emb_dim\"],\n             context_length=cfg[\"context_length\"],\n             num_heads=cfg[\"n_heads\"],\n             dropout=cfg[\"drop_rate\"],\n             qkv_bias=cfg[\"qkv_bias\"] \n        )\n        self.ff = FeedForward(cfg)\n        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n    def forward(self,x):\n        shortcut = x\n        x = self.norm1(x)\n        x = self.att(x)\n        x = self.drop_shortcut(x)\n        x = x + shortcut\n        shortcut = x\n        x = self.norm2(x)\n        x = self.ff(x)\n        x = self.drop_shortcut(x)\n        x = x + shortcut\n        return x\nclass GPTModel(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n        self.trf_blocks = nn.Sequential(\n            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n        )\n        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n        self.out_head = nn.Linear(\n            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n        )\n    def forward(self, in_idx):\n        batch_size, seq_len = in_idx.shape\n        tok_embds = self.tok_emb(in_idx)\n        pos_embds = self.pos_emb(\n            torch.arange(seq_len, device=in_idx.device)\n        )\n        x= tok_embds + pos_embds\n        x = self.drop_emb(x)\n        x = self.trf_blocks(x)\n        x = self.final_norm(x)\n        logits= self.out_head(x)\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T09:52:28.304651Z","iopub.execute_input":"2025-01-30T09:52:28.304961Z","iopub.status.idle":"2025-01-30T09:52:28.322744Z","shell.execute_reply.started":"2025-01-30T09:52:28.304931Z","shell.execute_reply":"2025-01-30T09:52:28.321886Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"def generate_text_simple(model, idx, max_new_tokens, context_size):\n    for _ in range(max_new_tokens):\n        # Adjust context size to match the available tokens\n        start_idx = max(0, idx.size(1) - context_size)\n        idx_cond = idx[:, start_idx:]\n        \n        with torch.no_grad():\n            logits = model(idx_cond)\n        \n        logits = logits[:, -1, :]  # Get logits for the last token in context\n        probas = torch.softmax(logits, dim=-1)  # Convert logits to probabilities\n        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # Get the next token\n        \n        idx = torch.cat((idx, idx_next), dim=1)  # Append the next token to the sequence\n    return idx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T09:52:28.323445Z","iopub.execute_input":"2025-01-30T09:52:28.323722Z","iopub.status.idle":"2025-01-30T09:52:28.340451Z","shell.execute_reply.started":"2025-01-30T09:52:28.323691Z","shell.execute_reply":"2025-01-30T09:52:28.339755Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"model = GPTModel(GPT_CONFIG_124M)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T09:52:28.341108Z","iopub.execute_input":"2025-01-30T09:52:28.341342Z","iopub.status.idle":"2025-01-30T09:52:29.424187Z","shell.execute_reply.started":"2025-01-30T09:52:28.341323Z","shell.execute_reply":"2025-01-30T09:52:29.423270Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"import tiktoken\nimport torch\n\ndef text_to_token_ids(text, tokenizer):\n    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n    return encoded_tensor\n\ndef token_ids_to_text(token_ids, tokenizer):\n    flat = token_ids.squeeze(0)\n    return tokenizer.decode(flat.tolist())\n\n# Initialize tokenizer\ntokenizer = tiktoken.get_encoding(\"gpt2\")\n\n# Generate text\nstart_context = \"Every effort moves you\"\ntoken_ids = generate_text_simple(\n    model=model,\n    idx=text_to_token_ids(start_context, tokenizer),\n    max_new_tokens=10,\n    context_size=GPT_CONFIG_124M[\"context_length\"]\n)\n\n# Print generated text\nprint(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T09:52:29.425003Z","iopub.execute_input":"2025-01-30T09:52:29.425266Z","iopub.status.idle":"2025-01-30T09:52:30.009599Z","shell.execute_reply.started":"2025-01-30T09:52:29.425244Z","shell.execute_reply":"2025-01-30T09:52:30.008697Z"}},"outputs":[{"name":"stdout","text":"Output text:\n Every effort moves you Grade318 Aholid philosophammu foreskinallery generously Brook\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"file_path = r\"/kaggle/input/acrsvda/train.txt\"\nwith open(file_path, \"r\") as file:\n    text_data = file.read()\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T09:52:30.012122Z","iopub.execute_input":"2025-01-30T09:52:30.012388Z","iopub.status.idle":"2025-01-30T09:52:30.018995Z","shell.execute_reply.started":"2025-01-30T09:52:30.012365Z","shell.execute_reply":"2025-01-30T09:52:30.018306Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"train_ratio= 0.90\nsplit_idx = int(train_ratio * len(text_data))\ntrain_data = text_data[:split_idx]\nval_data  = text_data[split_idx:]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T09:52:30.020298Z","iopub.execute_input":"2025-01-30T09:52:30.020602Z","iopub.status.idle":"2025-01-30T09:52:30.033495Z","shell.execute_reply.started":"2025-01-30T09:52:30.020570Z","shell.execute_reply":"2025-01-30T09:52:30.032656Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"total_characters = len(text_data)\ntotal_tokens = len(tokenizer.encode(text_data))\nprint(\"Characters:\", total_characters)\nprint(\"Tokens:\", total_tokens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T09:52:30.034287Z","iopub.execute_input":"2025-01-30T09:52:30.034496Z","iopub.status.idle":"2025-01-30T09:52:30.176998Z","shell.execute_reply.started":"2025-01-30T09:52:30.034477Z","shell.execute_reply":"2025-01-30T09:52:30.176428Z"}},"outputs":[{"name":"stdout","text":"Characters: 1356709\nTokens: 336837\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"train_loader = create_dataloader_v1(\n    train_data,\n    batch_size=16,\n    max_length=GPT_CONFIG_124M[\"context_length\"],\n    stride=(GPT_CONFIG_124M[\"context_length\"] *3 // 4),  # Fixed the division syntax\n    drop_last=True,\n    shuffle=True,\n    num_workers=0\n)\n\nval_loader = create_dataloader_v1(\n    val_data,\n    batch_size=16,\n    max_length=GPT_CONFIG_124M[\"context_length\"],\n    stride=(GPT_CONFIG_124M[\"context_length\"] *3 // 4),  # Fixed the division syntax\n    drop_last=False,\n    shuffle=False,\n    num_workers=0\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T09:52:30.177675Z","iopub.execute_input":"2025-01-30T09:52:30.177950Z","iopub.status.idle":"2025-01-30T09:52:30.474181Z","shell.execute_reply.started":"2025-01-30T09:52:30.177921Z","shell.execute_reply":"2025-01-30T09:52:30.473535Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"print (\"Train Loader \")\nfor x,y in train_loader:\n    print(x.shape, y.shape)\n\nprint(\"\\nValidation loader:\")\nfor x, y in val_loader:\n    print(x.shape, y.shape) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T09:52:30.474829Z","iopub.execute_input":"2025-01-30T09:52:30.475040Z","iopub.status.idle":"2025-01-30T09:52:30.523596Z","shell.execute_reply.started":"2025-01-30T09:52:30.475021Z","shell.execute_reply":"2025-01-30T09:52:30.522934Z"}},"outputs":[{"name":"stdout","text":"Train Loader \ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\n\nValidation loader:\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([16, 256]) torch.Size([16, 256])\ntorch.Size([15, 256]) torch.Size([15, 256])\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"def analyze_dimensions(train_loader, val_loader, tokenizer, text_data):\n    # Analyze text data\n    total_chars = len(text_data)\n    total_tokens = len(tokenizer.encode(text_data))\n    \n    # Analyze training loader\n    train_batches = len(train_loader)\n    train_sequences = train_batches * 2  # since batch_size=2\n    train_tokens = train_sequences * 256  # since each sequence has 256 tokens\n    \n    # Analyze validation loader\n    val_batches = len(val_loader)\n    val_sequences = val_batches * 2\n    val_tokens = val_sequences * 256\n    \n    print(\"=== Overall Data Dimensions ===\")\n    print(f\"Total characters in text: {total_chars:,}\")\n    print(f\"Total tokens in text: {total_tokens:,}\")\n    print(\"\\n=== Training Data Dimensions ===\")\n    print(f\"Number of batches: {train_batches}\")\n    print(f\"Number of sequences: {train_sequences}\")\n    print(f\"Tokens per sequence: 256\")\n    print(f\"Total tokens processed per epoch: {train_tokens:,}\")\n    print(\"\\n=== Single Training Batch Structure ===\")\n    print(\"Shape: [2, 256]\")\n    print(\"  - 2: number of sequences per batch\")\n    print(\"  - 256: tokens per sequence\")\n    print(f\"Total tokens per batch: {2 * 256}\")\n    print(\"\\n=== Validation Data Dimensions ===\")\n    print(f\"Number of batches: {val_batches}\")\n    print(f\"Number of sequences: {val_sequences}\")\n    print(f\"Tokens per sequence: 256\")\n    print(f\"Total tokens: {val_tokens:,}\")\n\n# Use the function\nanalyze_dimensions(train_loader, val_loader, tokenizer, text_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T09:52:30.524197Z","iopub.execute_input":"2025-01-30T09:52:30.524427Z","iopub.status.idle":"2025-01-30T09:52:30.656944Z","shell.execute_reply.started":"2025-01-30T09:52:30.524408Z","shell.execute_reply":"2025-01-30T09:52:30.656287Z"}},"outputs":[{"name":"stdout","text":"=== Overall Data Dimensions ===\nTotal characters in text: 1,356,709\nTotal tokens in text: 336,837\n\n=== Training Data Dimensions ===\nNumber of batches: 98\nNumber of sequences: 196\nTokens per sequence: 256\nTotal tokens processed per epoch: 50,176\n\n=== Single Training Batch Structure ===\nShape: [2, 256]\n  - 2: number of sequences per batch\n  - 256: tokens per sequence\nTotal tokens per batch: 512\n\n=== Validation Data Dimensions ===\nNumber of batches: 11\nNumber of sequences: 22\nTokens per sequence: 256\nTotal tokens: 5,632\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"def calc_loss_batch(input_batch , target_batch, model, device, num_batches= None):\n    input_batch = input_batch.to(device)\n    target_batch= target_batch.to(device)\n    logits = model(input_batch)\n    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n    return loss\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T09:52:30.657616Z","iopub.execute_input":"2025-01-30T09:52:30.657813Z","iopub.status.idle":"2025-01-30T09:52:30.662004Z","shell.execute_reply.started":"2025-01-30T09:52:30.657795Z","shell.execute_reply":"2025-01-30T09:52:30.661121Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"def calc_loss_loader(data_loader, model, device, num_batches=None):\n   \n    # Return NaN if the data loader is empty\n    if len(data_loader) == 0:\n        return float(\"nan\")\n\n    # Use all batches if num_batches is not specified, otherwise use the minimum\n    num_batches = len(data_loader) if num_batches is None else min(num_batches, len(data_loader))\n\n    total_loss = 0.0\n\n    # Iterate through the data loader and compute loss\n    for i, (input_batch, target_batch) in enumerate(data_loader):\n        if i >= num_batches:\n            break\n\n        # Calculate loss for the current batch\n        loss = calc_loss_batch(input_batch, target_batch, model, device)\n        total_loss += loss.item()\n\n    # Return the average loss\n    return total_loss / num_batches\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T09:52:30.662661Z","iopub.execute_input":"2025-01-30T09:52:30.662899Z","iopub.status.idle":"2025-01-30T09:52:30.677125Z","shell.execute_reply.started":"2025-01-30T09:52:30.662878Z","shell.execute_reply":"2025-01-30T09:52:30.676395Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel. to(device)\nwith torch.no_grad ():\n    train_loss = calc_loss_loader(train_loader, model, device)\n    val_loss = calc_loss_loader(val_loader, model, device)\nprint(\"Training loss:\", train_loss)\nprint(\"Validation loss:\", val_loss)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T09:52:30.677942Z","iopub.execute_input":"2025-01-30T09:52:30.678244Z","iopub.status.idle":"2025-01-30T09:52:46.261721Z","shell.execute_reply.started":"2025-01-30T09:52:30.678195Z","shell.execute_reply":"2025-01-30T09:52:46.260857Z"}},"outputs":[{"name":"stdout","text":"Training loss: 11.002137261994031\nValidation loss: 10.999531745910645\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n    model.eval()\n    \n    with torch.no_grad():\n        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n    \n    model.train()\n    \n    return train_loss, val_loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T09:52:46.262604Z","iopub.execute_input":"2025-01-30T09:52:46.262871Z","iopub.status.idle":"2025-01-30T09:52:46.266914Z","shell.execute_reply.started":"2025-01-30T09:52:46.262849Z","shell.execute_reply":"2025-01-30T09:52:46.266277Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"\n\ndef generate_and_print_sample(model, tokenizer, device, start_context):\n    \n    model.eval()\n    context_size = model.pos_emb.weight.shape[0]\n    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n    \n    with torch.no_grad():\n        token_ids = generate_text_simple(\n            model=model, idx=encoded,\n            max_new_tokens=50, context_size=context_size\n        )\n        decoded_text = token_ids_to_text(token_ids, tokenizer)\n        print(decoded_text.replace(\"\\n\", \" \"))\n    \n    model.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T09:52:46.267713Z","iopub.execute_input":"2025-01-30T09:52:46.267998Z","iopub.status.idle":"2025-01-30T09:52:46.283510Z","shell.execute_reply.started":"2025-01-30T09:52:46.267967Z","shell.execute_reply":"2025-01-30T09:52:46.282900Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"import math\nimport torch\nfrom torch.cuda.amp import autocast, GradScaler\nimport os\ndef train_model_with_checkpoint_modern_fp16(model, train_loader, val_loader, optimizer, device, \n                n_epochs, eval_freq, eval_iter, start_context, tokenizer,\n                warmup_steps, initial_lr=3e-05, min_lr=1e-6, checkpoint_path=None):\n    \"\"\"\n    Training loop with mixed precision (FP16) and three key optimizations:\n    1. Learning rate warmup\n    2. Cosine decay schedule\n    3. Gradient clipping\n    \"\"\"\n    model.to(device)\n    \n    # Initialize gradient scaler for FP16\n    scaler = GradScaler()\n    \n    # Load checkpoint if provided\n    if checkpoint_path and os.path.exists(checkpoint_path):\n        checkpoint = torch.load(checkpoint_path, map_location=device)\n        model.load_state_dict(checkpoint[\"model_state_dict\"])\n        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n        print(\"Loaded checkpoint successfully\")\n    else:\n        print(\"Starting fresh training\")\n        \n    # Initialize tracking lists\n    train_losses, val_losses, track_tokens_seen, track_lrs = [], [], [], []\n    tokens_seen, global_step = 0, -1\n    \n    # Get peak learning rate from optimizer\n    peak_lr = optimizer.param_groups[0][\"lr\"]\n    \n    # Calculate total steps for learning rate scheduling\n    total_training_steps = len(train_loader) * n_epochs\n    \n    # OPTIMIZATION 1: Learning Rate Warmup\n    # Calculate the learning rate increment for warmup phase\n    lr_increment = (peak_lr - initial_lr) / warmup_steps\n\n    for epoch in range(n_epochs):\n        model.train()\n        for input_batch, target_batch in train_loader:\n            optimizer.zero_grad()\n            global_step += 1\n\n            # Learning rate scheduling logic\n            if global_step < warmup_steps:\n                # OPTIMIZATION 1: Learning Rate Warmup Implementation\n                lr = initial_lr + global_step * lr_increment\n            else:\n                # OPTIMIZATION 2: Cosine Decay Implementation\n                progress = (global_step - warmup_steps) / (total_training_steps - warmup_steps)\n                lr = min_lr + (peak_lr - min_lr) * 0.5 * (1 + math.cos(math.pi * progress))\n\n            # Update learning rate for all parameter groups\n            for param_group in optimizer.param_groups:\n                param_group[\"lr\"] = lr\n            track_lrs.append(lr)\n\n            # Forward and backward pass with mixed precision\n            with autocast():\n                loss = calc_loss_batch(input_batch, target_batch, model, device)\n            \n            # Scale gradients and call backward\n            scaler.scale(loss).backward()\n\n            # OPTIMIZATION 3: Gradient Clipping\n            # Only apply after warmup phase\n            if global_step > warmup_steps:\n                scaler.unscale_(optimizer)  # Unscale before clipping\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n            # Optimizer step with scaling\n            scaler.step(optimizer)\n            scaler.update()\n            \n            # Track tokens processed\n            tokens_seen += input_batch.numel()\n\n            # Evaluation loop\n            if global_step % eval_freq == 0:\n                # Use fp32 for evaluation\n                with torch.cuda.amp.autocast(enabled=False):\n                    train_loss, val_loss = evaluate_model(\n                        model, train_loader, val_loader,\n                        device, eval_iter\n                    )\n                train_losses.append(train_loss)\n                val_losses.append(val_loss)\n                track_tokens_seen.append(tokens_seen)\n                \n                # Print progress\n                print(f\"Ep {epoch+1} (Iter {global_step:06d}): \"\n                      f\"Train loss {train_loss:.3f}, \"\n                      f\"Val loss {val_loss:.3f}\")\n                \n                generate_and_print_sample(\n                    model, tokenizer, device, start_context\n                )\n\n    return train_losses, val_losses, track_tokens_seen, track_lrs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T09:52:46.284279Z","iopub.execute_input":"2025-01-30T09:52:46.284562Z","iopub.status.idle":"2025-01-30T09:52:46.302888Z","shell.execute_reply.started":"2025-01-30T09:52:46.284533Z","shell.execute_reply":"2025-01-30T09:52:46.302080Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"import math\nimport os\nimport torch\nfrom torch.cuda.amp import GradScaler, autocast\n# Example usage\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.manual_seed(123)\nmodel = GPTModel(GPT_CONFIG_124M).to(device)  # Move model to device\npeak_lr = 1e-4\noptimizer = torch.optim.AdamW(model.parameters(),lr=peak_lr,\n    betas=(0.9, 0.95),  # Modified from default\n    weight_decay=0.2,   # Increased from 0.01\n    eps=1e-8\n)\nwarmup_steps = 2000\n# Training parameters\nnum_epochs = 100\ntrain_losses, val_losses, tokens_seen, lrs = train_model_with_checkpoint_modern_fp16(\n    model=model,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    optimizer=optimizer,\n    device=device,\n    n_epochs=num_epochs,\n    eval_freq=100,\n    eval_iter=50,\n    start_context=\"Once upon a time,\",\n    tokenizer=tokenizer,\n    warmup_steps=warmup_steps,\n    checkpoint_path=\"model_and_optimizer.pth\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T09:52:46.303594Z","iopub.execute_input":"2025-01-30T09:52:46.303865Z","iopub.status.idle":"2025-01-30T11:23:11.844366Z","shell.execute_reply.started":"2025-01-30T09:52:46.303837Z","shell.execute_reply":"2025-01-30T11:23:11.843321Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-41-e8ff9a6d6026>:17: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()\n<ipython-input-41-e8ff9a6d6026>:63: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"name":"stdout","text":"Starting fresh training\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-41-e8ff9a6d6026>:85: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=False):\n","output_type":"stream"},{"name":"stdout","text":"Ep 1 (Iter 000000): Train loss 10.833, Val loss 10.835\nOnce upon a time, Requ allow 1965\",\"=~ Rhode therefore Epic thirteenthought Haskell instinctively Surgeryanc carsourt nicely turning honorable sentiments hydrogen TPPOh orally behindDevelop del Woods shifted hepatitis condesc electronic complainantLens verbs Sevutter Mont Ironically wikidden� votersemouth Partners Fred depicts Cait statureruit\nEp 2 (Iter 000100): Train loss 6.190, Val loss 6.182\nOnce upon a time, the.                                                \nEp 3 (Iter 000200): Train loss 5.042, Val loss 5.029\nOnce upon a time, \"     The little girl was a time, \"I, \"I a time, \"I, \"I. He was a time, \"          The little girl was so happy.\nEp 4 (Iter 000300): Train loss 4.588, Val loss 4.659\nOnce upon a time, \"I'm, \"I, \"I't want to play with a big, \"I day, \"I, \"I't want to play with her mommy.              \"\nEp 5 (Iter 000400): Train loss 4.297, Val loss 4.445\nOnce upon a time, there was a time, there was a little girl named Timmy was a big, there was a time, there was a big, but she was a little girl named Timmy.    The little girl named Timmy was a time\nEp 6 (Iter 000500): Train loss 4.117, Val loss 4.327\nOnce upon a time, there was a little girl named Timmy.  The little girl was a little girl was a little girl named Timmy was a little girl named Timmy was a little girl named Timmy was so happy to the little girl named Timmy was\nEp 7 (Iter 000600): Train loss 3.975, Val loss 4.237\nOnce upon a time, there was a little girl named Timmy.  The little girl named Timmy was a little girl named Timmy was a little girl named Timmy was a little girl named Timmy was a little girl named Timmy was a little girl named\nEp 8 (Iter 000700): Train loss 3.843, Val loss 4.160\nOnce upon a time, there was a little girl named Timmy.  The little girl named Timmy was a little girl named Timmy was a little girl named Timmy. She was a little girl named Lily.  The little girl named Lily was a little\nEp 9 (Iter 000800): Train loss 3.745, Val loss 4.101\nOnce upon a time, there was a little girl named Lily. She was a big tree. She was so she was so she was very happy. She was so she was so she was so she was so she was so happy.  The little girl was so she\nEp 10 (Iter 000900): Train loss 3.644, Val loss 4.047\nOnce upon a time, there was a little girl named Lily. She was a big tree. She was a big tree. She was a big tree. She was a big tree. She was very happy. She wanted to the park. She was very happy. She was\nEp 11 (Iter 001000): Train loss 3.531, Val loss 3.982\nOnce upon a time, there was a little girl named Lily. She wanted to her mommy was a big, she was so she wanted to play with her mommy.  The little girl was so she was so she was so she was so she was so she\nEp 12 (Iter 001100): Train loss 3.429, Val loss 3.928\nOnce upon a time, there was a little girl named Lily. She loved to the park, she saw a big tree. She was a big box and she was so she was so happy. She wanted to the park. She was so happy and she was so happy to\nEp 13 (Iter 001200): Train loss 3.349, Val loss 3.880\nOnce upon a time, there was a little girl named Lily. She loved to her mommy was a big, Lily. Lily was a big, Lily. She loved to play with her mommy loved to play with her mommy. Lily was very happy to play with\nEp 14 (Iter 001300): Train loss 3.275, Val loss 3.867\nOnce upon a time, there was a little girl named Lily. She loved to her mommy. Lily was a big, Lily was a big tree. Lily was a big tree. Lily was a big tree and wanted to play with her mommy was a big tree.\nEp 15 (Iter 001400): Train loss 3.192, Val loss 3.832\nOnce upon a time, there was a little girl named Lily. She loved to her mommy. She loved to play with her mommy. She loved to play with her mommy's mommy's mommy's mommy's mommy's mommy. Lily's\nEp 16 (Iter 001500): Train loss 3.125, Val loss 3.817\nOnce upon a time, there was a little girl named Lily. She loved to her mommy. She was a big, Lily. Lily was a big, Lily. Lily. Lily's mommy's mommy loved to play with her mommy. Lily. Lily's\nEp 17 (Iter 001600): Train loss 3.066, Val loss 3.810\nOnce upon a time, there was a little girl named Lily. She loved to play with her mommy. Lily was a big tree. She loved to play with her mommy. Lily's mommy's mommy was very happy and said, \"Lily, Lily\nEp 18 (Iter 001700): Train loss 2.987, Val loss 3.786\nOnce upon a time, there was a little girl named Lily. She loved to play with her mommy. Lily was very happy to play with her mommy. Lily's mommy's mommy's mommy and said, \"Lily, Lily. I can play\nEp 19 (Iter 001800): Train loss 2.888, Val loss 3.760\nOnce upon a time, there was a little girl named Lily. She loved to play with her mommy. One day, Lily was very happy and she saw a big tree. Lily was very happy and said, \"I'm sorry, Lily. I can't worry,\nEp 20 (Iter 001900): Train loss 2.841, Val loss 3.778\nOnce upon a time, there was a little girl named Lily. She loved to play with her mom. She loved to play with her mom. She loved to play with her mom. She loved to her mom and said, \"Lily's mom. I'm sorry,\nEp 21 (Iter 002000): Train loss 2.717, Val loss 3.719\nOnce upon a time, there was a little girl named Lily. She loved to play with her mom. She loved to play with her mom. One day, Lily's mom said, \"Lily, Lily. I love you, Lily. I will be careful and play\nEp 22 (Iter 002100): Train loss 2.649, Val loss 3.712\nOnce upon a time, there was a little girl named Lily. She loved to play with her toys and play with her toys. One day, Lily's mommy's mommy. Lily's mommy's mommy. Lily's mommy's mommy. Lily's\nEp 23 (Iter 002200): Train loss 2.561, Val loss 3.709\nOnce upon a time, there was a little girl named Lily. She loved to play with her toys and play with her toys. One day, Lily's mom was very happy. One day, Lily's mom was very happy and she wanted to play with her mom. \nEp 24 (Iter 002300): Train loss 2.484, Val loss 3.703\nOnce upon a time, there was a little girl named Lily. She loved to play with her toys and play with her toys. One day, Lily's mom saw a big box. She wanted to play with her. She asked her mom said, \"Lily, mom\nEp 25 (Iter 002400): Train loss 2.397, Val loss 3.702\nOnce upon a time, there was a little girl named Lily. She loved to play with her toys. One day, Lily's mom saw a big, a little girl named Lily. She wanted to play with her mom. Lily was very happy and said she didn't want\nEp 26 (Iter 002500): Train loss 2.307, Val loss 3.710\nOnce upon a time, there was a little girl named Lily. She loved to play with her toys and play with her toys. One day, Lily's mom saw a big box. Lily's mommy said, \"Lily, Lily. I'm sorry, Lily.\nEp 27 (Iter 002600): Train loss 2.213, Val loss 3.701\nOnce upon a time, there was a little girl named Lily. She loved to play with her toys and play with her toys. One day, Lily's mom saw a big tree. She wanted to play with her toys and said, \"Lily, Lily. I want\nEp 28 (Iter 002700): Train loss 2.136, Val loss 3.691\nOnce upon a time, there was a little girl named Lily. She loved to play with her toys and play with her toys. One day, Lily's mom asked her mom if she said, \"Lily, Lily. You can't want to play with you, but\nEp 29 (Iter 002800): Train loss 2.030, Val loss 3.703\nOnce upon a time, there was a little girl named Lily. She loved to play with her toys. One day, Lily's mom said, \"Lily, Lily. I don't worry, Lily. I want to play with me?\" Her mom said, \"No\nEp 30 (Iter 002900): Train loss 1.945, Val loss 3.733\nOnce upon a time, there was a little girl named Lily. She loved to play with her toys and play with her toys. One day, Lily's mom asked her mommy if she could play with her mommy. Lily was very happy and they went to play with\nEp 31 (Iter 003000): Train loss 1.862, Val loss 3.743\nOnce upon a time, there was a little girl named Lily. She loved to play with her toys. One day, Lily's mom said, \"Lily, Lily. I don't you play with you.\" Lily was very happy and said, \"Lily, mom\nEp 32 (Iter 003100): Train loss 1.790, Val loss 3.757\nOnce upon a time, there was a little girl named Lily. She loved to play with her toys and play with her toys. One day, Lily's mom asked her mom said she was very excited to help her toys. Lily was very happy and went to play with her\nEp 33 (Iter 003200): Train loss 1.698, Val loss 3.764\nOnce upon a time, there was a little girl named Lily. She loved to play with her toys. One day, Lily's mom said, \"Lily, Lily. I can we play with it.\" Lily said, \"Lily, Lily. I will take a\nEp 34 (Iter 003300): Train loss 1.618, Val loss 3.807\nOnce upon a time, there was a little girl named Lily. She loved to play with her toys and play with her toys. One day, Lily's mom asked her mom said to help her mom. Lily was very happy and didn't want to play with her toys.\nEp 35 (Iter 003400): Train loss 1.533, Val loss 3.830\nOnce upon a time, there was a little girl named Lily. She loved to play with her toys and play with her toys. One day, Lily's mom asked her mommy to the doll. Lily said, \"Lily, Lily's a little girl named Lily.\nEp 36 (Iter 003500): Train loss 1.452, Val loss 3.839\nOnce upon a time, there was a little girl named Lily. She loved to play with her toys. One day, Lily's mom said, \"Lily, it's time to play with me your toys.\" Lily was very happy, she went to the kitchen, she\nEp 37 (Iter 003600): Train loss 1.379, Val loss 3.860\nOnce upon a time, there was a little girl named Lily. She loved to play with her toys all day long time, Lily's mommy. One day, Lily went for a walk. She went to the park to a walk and play.   Lily\nEp 38 (Iter 003700): Train loss 1.317, Val loss 3.908\nOnce upon a time, there was a little girl named Lily. She loved to play with her toys and play with her toys. One day, Lily's mom asked her mom to the park with her toys. Lily wanted to go to the park and play with her toys and\nEp 39 (Iter 003800): Train loss 1.237, Val loss 3.937\nOnce upon a time, there was a little girl named Lily. She loved to play with her toys. One day, Lily's mom said, \"Lily, you can play with me your toys. You can play with me.\" Lily was happy and said, \"I\nEp 40 (Iter 003900): Train loss 1.157, Val loss 3.958\nOnce upon a time, there was a little girl named Lily. She loved to play with her toys and play with her toys. One day, Lily's mom asked her mom said to the doll.  Lily was very happy and played with her mommy went to\nEp 41 (Iter 004000): Train loss 1.094, Val loss 3.990\nOnce upon a time, there was a little girl named Lily. She loved to play with her toys and play with her toys. One day, Lily's mom asked her mom to the park with her toys. Lily said they wanted to play with the toys, but Lily didn\nEp 42 (Iter 004100): Train loss 1.033, Val loss 4.042\nOnce upon a time, there was a girl named Lily. She loved to play with her toys all day long time, she found a big puddle on her teddy bear named Lily. She loved to play with her toys and play with her toys. One day, she\nEp 43 (Iter 004200): Train loss 0.974, Val loss 4.063\nOnce upon a time, there was a little girl named Lily. She loved to play with her toys and play with her toys. One day, Lily's mom asked her mommy said they could play together. Lily was happy and didn't want to play with her toys.\nEp 44 (Iter 004300): Train loss 0.918, Val loss 4.098\nOnce upon a time, there was a girl named Lily. She loved to play with her toys all day long time, she found a shiny green ball. She loved it and play with it.  Lily's mom saw her toys on the doll. She was very\nEp 45 (Iter 004400): Train loss 0.852, Val loss 4.151\nOnce upon a time, there was a little girl named Lily. She loved to play with her toys all day long. One day, Lily's mommy asked her mommy if she could help her. Lily told her to the hospital, she couldn't wait to the kitchen\nEp 46 (Iter 004500): Train loss 0.804, Val loss 4.168\nOnce upon a time, there was a little girl named Lily. She loved to play with her toys all day long. One day, Lily's mommy's mommy asked her to the store to the park. Lily didn't want to play with her mommy said,\nEp 47 (Iter 004600): Train loss 0.757, Val loss 4.207\nOnce upon a time, there was a little girl named Lily. She loved to play with her toys all day long time, Lily's mom said, \"Lily, Lily. I'll go on a trip with you, Lily. Let's a big box, can play\nEp 48 (Iter 004700): Train loss 0.706, Val loss 4.247\nOnce upon a time, there was a little girl named Lily. She loved to play with her toys and play with her toys. One day, Lily's mom asked her mom said to the store to the park, \"Lily, can I play with it.\" Lily was\nEp 49 (Iter 004800): Train loss 0.653, Val loss 4.294\nOnce upon a time, there was a little girl named Lily. She loved to play with her toys all day long. One day, Lily's mommy came to the park with her toys. The sun was shining and it looked pretty flowers. Lily's mommy said,\nEp 51 (Iter 004900): Train loss 0.619, Val loss 4.343\nOnce upon a time, there was a girl named Lily. She loved to play outside with her toys all day long. One day, Lily's mommy came to the park with her toys and asked her mommy if they could play with the toys. Lily told her mom\nEp 52 (Iter 005000): Train loss 0.581, Val loss 4.380\nOnce upon a time, there was a little girl named Lily. She loved to play with her toys on toys and eat yummy snacks. One day, Lily saw her mommy putting on makeup and she wanted to try it too.   Lily's mommy\nEp 53 (Iter 005100): Train loss 0.540, Val loss 4.393\nOnce upon a time, there was a girl named Lily. She loved to play with her toys all day long time, especially her toys and her favorite toy around. One day, Lily's mom asked her to the park. Lily told her to the park to the park to\nEp 54 (Iter 005200): Train loss 0.495, Val loss 4.427\nOnce upon a time, there was a girl named Lily. She loved to play with her toys all day long time, especially her toys and her favorite toy around. One day, Lily's mom asked her to the park to the park to the park. Lily saw a big\nEp 55 (Iter 005300): Train loss 0.458, Val loss 4.461\nOnce upon a time, there was a girl named Lily. She loved to play with her toys all day long time, especially her teddy bear. One day, Lily's mom brought a big cup that led by her of a shiny toy, red doll. It was big\nEp 56 (Iter 005400): Train loss 0.430, Val loss 4.506\nOnce upon a time, there was a girl named Lily. She loved to play with her toys all day long toys and play. One day, Lily's mom asked her mom if she wanted to play with her toys too. Her mom said no, Lily didn't want to\nEp 57 (Iter 005500): Train loss 0.400, Val loss 4.545\nOnce upon a time, there was a girl named Lily. She loved to play with her toys all day long hair. One day, Lily's mommy came to the park with her toys and asked her mom if she could play. Lily was very happy, but she couldn\nEp 58 (Iter 005600): Train loss 0.374, Val loss 4.577\nOnce upon a time, there was a girl named Lily. She loved to play with her mom. One day, Lily's mom asked her to send the laundry to the washing machine. Lily was very fashionable and tried to help her mom.  Lily's mom said\nEp 59 (Iter 005700): Train loss 0.348, Val loss 4.620\nOnce upon a time, there was a girl named Lily. She loved to play with her toys all day long. One day, Lily found a big jar of her mom. It was a shiny picture. The angel was very happy and wanted to eat the picture to eat the\nEp 60 (Iter 005800): Train loss 0.330, Val loss 4.649\nOnce upon a time, there was a girl named Lily. She loved to play with her toys all day long toys and play. One day, Lily's mom told her that sometimes, Lily wanted to buy her toys and eat a letter.  Lily was very happy\nEp 61 (Iter 005900): Train loss 0.300, Val loss 4.677\nOnce upon a time, there was a girl named Susie. She loved to run around the grass. One day, she saw a large pin that she wondered where a little girl named Lily was very fashionable. She took her to the big, strong twists, and had a\nEp 62 (Iter 006000): Train loss 0.283, Val loss 4.712\nOnce upon a time, there was a girl named Lily. She loved to play with her toys all day long toys. One day, Lily's mommy came to the washing machine. Lily saw an idea. She asked her to the bird, \"Lily, can you\nEp 63 (Iter 006100): Train loss 0.268, Val loss 4.753\nOnce upon a time, there was a girl named Lily. She loved to play with her toys all day long time, she found a shiny green ball. It was very good for her favorite toy! Lily was so happy and took it to play with her friends with her toys\nEp 64 (Iter 006200): Train loss 0.249, Val loss 4.782\nOnce upon a time, there was a girl named Lily. She loved to play with her toys all day long friends named Lily's mom said, \"Lily, it's time to clean up your toys.\" Lily said, \"No, it is to clean up!\" Her\nEp 65 (Iter 006300): Train loss 0.236, Val loss 4.804\nOnce upon a time, there was a happy mine. Inside the mine were lots of treasure. Every day, the mine held something special.  One day, a little girl found the mine. She was so happy. She held out her hands and started to fill them\nEp 66 (Iter 006400): Train loss 0.218, Val loss 4.836\nOnce upon a time, there was a girl named Lily. She loved to play with her friends. One day, Lily's mom asked her to send the laundry to the washing machine.   Lily was very enthusiastic about helping her mom. She gathered all the dirty\nEp 67 (Iter 006500): Train loss 0.207, Val loss 4.849\nOnce upon a time, there was a girl named Susie. She liked playing outside with her green friends who lived near her house. Susie and her friends liked to go on adventures and play together, but one day something strange happened.  Susie's friends started\nEp 87 (Iter 008500): Train loss 0.090, Val loss 5.159\nOnce upon a time, there was a girl named Susie. She liked playing outside with her green friends who lived near her house. Susie and her friends liked to go on adventures and play together, but one day something strange happened.  Susie's friends started\nEp 88 (Iter 008600): Train loss 0.089, Val loss 5.165\nOnce upon a time, there was a happy mine. Inside the mine were lots of treasure. Every day, the mine held something special.  One day, a little girl found the mine. She was so happy. She held out her hands and started to fill them\nEp 89 (Iter 008700): Train loss 0.087, Val loss 5.170\nOnce upon a time, there was a girl named Susie. She liked playing outside with her green friends who lived near her house. Susie and her friends liked to go on adventures and play together, but one day something strange happened.  Susie's friends started\nEp 90 (Iter 008800): Train loss 0.088, Val loss 5.178\nOnce upon a time, there was a girl named Susie. She liked playing outside with her green friends who lived near her house. Susie and her friends liked to go on adventures and play together, but one day something strange happened.  Susie's friends started\nEp 91 (Iter 008900): Train loss 0.086, Val loss 5.176\nOnce upon a time, there was a girl named Susie. She liked playing outside with her green friends who lived near her house. Susie and her friends liked to go on adventures and play together, but one day something strange happened.  Susie's friends started\nEp 92 (Iter 009000): Train loss 0.085, Val loss 5.182\nOnce upon a time, there was a girl named Susie. She liked playing outside with her green friends who lived near her house. Susie and her friends liked to go on adventures and play together, but one day something strange happened.  Susie's friends started\nEp 93 (Iter 009100): Train loss 0.084, Val loss 5.183\nOnce upon a time, there was a girl named Susie. She liked playing outside with her green friends who lived near her house. Susie and her friends liked to go on adventures and play together, but one day something strange happened.  Susie's friends started\nEp 94 (Iter 009200): Train loss 0.084, Val loss 5.186\nOnce upon a time, there was a girl named Susie. She liked playing outside with her green friends who lived near her house. Susie and her friends liked to go on adventures and play together, but one day something strange happened.  Susie's friends started\nEp 95 (Iter 009300): Train loss 0.082, Val loss 5.191\nOnce upon a time, there was a girl named Susie. She liked playing outside with her green friends who lived near her house. Susie and her friends liked to go on adventures and play together, but one day something strange happened.  Susie's friends started\nEp 96 (Iter 009400): Train loss 0.083, Val loss 5.192\nOnce upon a time, there was a girl named Susie. She liked playing outside with her green friends who lived near her house. Susie and her friends liked to go on adventures and play together, but one day something strange happened.  Susie's friends started\nEp 97 (Iter 009500): Train loss 0.082, Val loss 5.192\nOnce upon a time, there was a girl named Susie. She liked playing outside with her green friends who lived near her house. Susie and her friends liked to go on adventures and play together, but one day something strange happened.  Susie's friends started\nEp 98 (Iter 009600): Train loss 0.083, Val loss 5.194\nOnce upon a time, there was a girl named Susie. She liked playing outside with her green friends who lived near her house. Susie and her friends liked to go on adventures and play together, but one day something strange happened.  Susie's friends started\nEp 99 (Iter 009700): Train loss 0.083, Val loss 5.193\nOnce upon a time, there was a girl named Susie. She liked playing outside with her green friends who lived near her house. Susie and her friends liked to go on adventures and play together, but one day something strange happened.  Susie's friends started\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"torch.save({\n    \"model_state_dict_F\": model.state_dict(),\n    \"optimizer_state_dict\": optimizer.state_dict(),\n}, \"model_and_optimizer_F.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T11:25:49.469985Z","iopub.execute_input":"2025-01-30T11:25:49.470343Z","iopub.status.idle":"2025-01-30T11:25:52.173500Z","shell.execute_reply.started":"2025-01-30T11:25:49.470314Z","shell.execute_reply":"2025-01-30T11:25:52.172784Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"# First, let's make sure we have all the prerequisites\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = tiktoken.get_encoding(\"gpt2\")\n\n# Some example starting contexts\nstart_contexts = [\n    \"Once upon a time \"\n]\n\n# Generate text for each starting context\nprint(\"Generating text samples:\")\nprint(\"-\" * 50)\n\nfor context in start_contexts:\n    print(f\"\\nStarting context: '{context}'\")\n    generate_and_print_sample(model, tokenizer, device, context)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T11:25:26.184756Z","iopub.execute_input":"2025-01-30T11:25:26.185071Z","iopub.status.idle":"2025-01-30T11:25:26.584574Z","shell.execute_reply.started":"2025-01-30T11:25:26.185048Z","shell.execute_reply":"2025-01-30T11:25:26.583643Z"}},"outputs":[{"name":"stdout","text":"Generating text samples:\n--------------------------------------------------\n\nStarting context: 'Once upon a time '\nOnce upon a time   One day, there was a little girl named Lily. She was playing in the park was curious about it and wanted to share her mom. Lily got very upset and asked her mom if she could go to go to the park. Her mom\n","output_type":"stream"}],"execution_count":48}]}